{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23d0278f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ditemukan 1 file:\n",
      "  - project/data/raw/energy/databebangedung - Sheet1.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/occupancy/DataOccupansi.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/temperature/survey_suhu_2024_daily2.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_opmc/ahu/MDP 3.2 _OPMC_ AHU L3.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_opmc/ahu/MDP 3.2 _OPMC_AHU L2.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_opmc/ahu/MDP 3.2 _OPMC_AHU L4.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_opmc/ahu/MDP 3.2 _OPMC_AHU L5.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_opmc/ahu/MDP 3.2 _OPMC_AHU L6.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_opmc/sdp/MDP 3.2 _OPMC_ SDP L3.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_opmc/sdp/MDP 3.2 _OPMC_SDP L1.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_opmc/sdp/MDP 3.2 _OPMC_SDP L2.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_opmc/sdp/MDP 3.2 _OPMC_SDP L4.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_opmc/sdp/MDP 3.2 _OPMC_SDP L5.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_opmc/sdp/MDP 3.2 _OPMC_SDP L6.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_witel/ahu/MDP 3. 1_Wittel_AHU L1.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_witel/ahu/MDP 3. 1_Wittel_AHU L2.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_witel/ahu/MDP 3. 1_Wittel_AHU L3.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_witel/ahu/MDP 3. 1_Wittel_AHU L4.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_witel/ahu/MDP 3. 1_Wittel_AHU L5.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_witel/ahu/MDP 3. 1_Wittel_AHU L6.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_witel/ahu/MDP 3. 1_Wittel_AHU L7.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_witel/ahu/MDP 3. 1_Wittel_AHU L8.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_witel/sdp/MDP 3. 1_Wittel_SDP L1.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_witel/sdp/MDP 3. 1_Wittel_SDP L2.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_witel/sdp/MDP 3. 1_Wittel_SDP L3.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_witel/sdp/MDP 3. 1_Wittel_SDP L4.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_witel/sdp/MDP 3. 1_Wittel_SDP L5.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_witel/sdp/MDP 3. 1_Wittel_SDP L6_SERVER NOC.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_witel/sdp/MDP 3. 1_Wittel_SDP L6.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_witel/sdp/MDP 3. 1_Wittel_SDP L7.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_witel/sdp/MDP 3. 1_Wittel_SDP L8.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_witel/utilities/MDP 3. 1_Wittel_CHILLER.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_witel/utilities/MDP 3. 1_Wittel_LIFT.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_witel/utilities/MDP 3. 1_Wittel_CHILLER.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/gedung_witel/utilities/MDP 3. 1_Wittel_SDP RECTI.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/panel_other/MDP 1 _DBT L1.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/panel_other/MDP 1 OSASE.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/panel_other/MDP 1_SDP DBT.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/panel_other/MDP 2 STO.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/panel_other/MDP 2_DBT AC.csv\n",
      "Ditemukan 1 file:\n",
      "  - project/data/raw/panel_other/MDP 2_PANEL RUANG SISCA L6.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "def baca_csv_pattern(pattern):\n",
    "    \"\"\"\n",
    "    Membaca file CSV berdasarkan pattern tertentu\n",
    "    Pattern examples:\n",
    "    - \"data/*.csv\" -> semua CSV di folder data\n",
    "    - \"sales_*.csv\" -> semua file yang dimulai dengan sales_\n",
    "    - \"**/*.csv\" -> semua CSV di semua subfolder (recursive)\n",
    "    \"\"\"\n",
    "    csv_files = glob.glob(pattern, recursive=True)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"Tidak ada file yang cocok dengan pattern!\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Ditemukan {len(csv_files)} file:\")\n",
    "    for file in csv_files:\n",
    "        print(f\"  - {file}\")\n",
    "    \n",
    "    dataframes = []\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        df['source_file'] = file\n",
    "        dataframes.append(df)\n",
    "    \n",
    "    return pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Contoh penggunaan\n",
    "# Membaca semua CSV di folder data\n",
    "data1 = baca_csv_pattern(\"project/data/raw/energy/databebangedung - Sheet1.csv\")\n",
    "\n",
    "# Membaca semua file sales\n",
    "data2 = baca_csv_pattern(\"project/data/raw/occupancy/DataOccupansi.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data3 = baca_csv_pattern(\"project/data/raw/temperature/survey_suhu_2024_daily2.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data4 = baca_csv_pattern(\"project/data/raw/gedung_opmc/ahu/MDP 3.2 _OPMC_ AHU L3.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data5 = baca_csv_pattern(\"project/data/raw/gedung_opmc/ahu/MDP 3.2 _OPMC_AHU L2.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data6 = baca_csv_pattern(\"project/data/raw/gedung_opmc/ahu/MDP 3.2 _OPMC_AHU L4.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data7 = baca_csv_pattern(\"project/data/raw/gedung_opmc/ahu/MDP 3.2 _OPMC_AHU L5.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data8 = baca_csv_pattern(\"project/data/raw/gedung_opmc/ahu/MDP 3.2 _OPMC_AHU L6.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data9 = baca_csv_pattern(\"project/data/raw/gedung_opmc/sdp/MDP 3.2 _OPMC_ SDP L3.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data10 = baca_csv_pattern(\"project/data/raw/gedung_opmc/sdp/MDP 3.2 _OPMC_SDP L1.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data11 = baca_csv_pattern(\"project/data/raw/gedung_opmc/sdp/MDP 3.2 _OPMC_SDP L2.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data12 = baca_csv_pattern(\"project/data/raw/gedung_opmc/sdp/MDP 3.2 _OPMC_SDP L4.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data13 = baca_csv_pattern(\"project/data/raw/gedung_opmc/sdp/MDP 3.2 _OPMC_SDP L5.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data14 = baca_csv_pattern(\"project/data/raw/gedung_opmc/sdp/MDP 3.2 _OPMC_SDP L6.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data15 = baca_csv_pattern(\"project/data/raw/gedung_witel/ahu/MDP 3. 1_Wittel_AHU L1.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data16 = baca_csv_pattern(\"project/data/raw/gedung_witel/ahu/MDP 3. 1_Wittel_AHU L2.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data17 = baca_csv_pattern(\"project/data/raw/gedung_witel/ahu/MDP 3. 1_Wittel_AHU L3.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data18 = baca_csv_pattern(\"project/data/raw/gedung_witel/ahu/MDP 3. 1_Wittel_AHU L4.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data19 = baca_csv_pattern(\"project/data/raw/gedung_witel/ahu/MDP 3. 1_Wittel_AHU L5.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data20 = baca_csv_pattern(\"project/data/raw/gedung_witel/ahu/MDP 3. 1_Wittel_AHU L6.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data21 = baca_csv_pattern(\"project/data/raw/gedung_witel/ahu/MDP 3. 1_Wittel_AHU L7.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data22 = baca_csv_pattern(\"project/data/raw/gedung_witel/ahu/MDP 3. 1_Wittel_AHU L8.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data23 = baca_csv_pattern(\"project/data/raw/gedung_witel/sdp/MDP 3. 1_Wittel_SDP L1.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data24 = baca_csv_pattern(\"project/data/raw/gedung_witel/sdp/MDP 3. 1_Wittel_SDP L2.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data25 = baca_csv_pattern(\"project/data/raw/gedung_witel/sdp/MDP 3. 1_Wittel_SDP L3.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data26 = baca_csv_pattern(\"project/data/raw/gedung_witel/sdp/MDP 3. 1_Wittel_SDP L4.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data27 = baca_csv_pattern(\"project/data/raw/gedung_witel/sdp/MDP 3. 1_Wittel_SDP L5.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data28 = baca_csv_pattern(\"project/data/raw/gedung_witel/sdp/MDP 3. 1_Wittel_SDP L6_SERVER NOC.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data29 = baca_csv_pattern(\"project/data/raw/gedung_witel/sdp/MDP 3. 1_Wittel_SDP L6.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data30 = baca_csv_pattern(\"project/data/raw/gedung_witel/sdp/MDP 3. 1_Wittel_SDP L7.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data31 = baca_csv_pattern(\"project/data/raw/gedung_witel/sdp/MDP 3. 1_Wittel_SDP L8.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data32 = baca_csv_pattern(\"project/data/raw/gedung_witel/utilities/MDP 3. 1_Wittel_CHILLER.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data33 = baca_csv_pattern(\"project/data/raw/gedung_witel/utilities/MDP 3. 1_Wittel_LIFT.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data34 = baca_csv_pattern(\"project/data/raw/gedung_witel/utilities/MDP 3. 1_Wittel_CHILLER.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data35 = baca_csv_pattern(\"project/data/raw/gedung_witel/utilities/MDP 3. 1_Wittel_SDP RECTI.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data36 = baca_csv_pattern(\"project/data/raw/panel_other/MDP 1 _DBT L1.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data37 = baca_csv_pattern(\"project/data/raw/panel_other/MDP 1 OSASE.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data38 = baca_csv_pattern(\"project/data/raw/panel_other/MDP 1_SDP DBT.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data39 = baca_csv_pattern(\"project/data/raw/panel_other/MDP 2 STO.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data40 = baca_csv_pattern(\"project/data/raw/panel_other/MDP 2_DBT AC.csv\")\n",
    "\n",
    "# Membaca semua CSV di semua subfolder\n",
    "data41 = baca_csv_pattern(\"project/data/raw/panel_other/MDP 2_PANEL RUANG SISCA L6.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc338bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏢  MENGGABUNGKAN DATA WITEL + METER MAPPING\n",
      "🎉 Selesai WITEL: 3435 records, mapped 3090 (90.0%)\n",
      " record_id  meter_id system_type level description category              source_file\n",
      "         1 251400321         AHU    L2      AHU L2      AHU MDP 3.2 _OPMC_AHU L2.csv\n",
      "         2 251400321         AHU    L2      AHU L2      AHU MDP 3.2 _OPMC_AHU L2.csv\n",
      "         3 251400321         AHU    L2      AHU L2      AHU MDP 3.2 _OPMC_AHU L2.csv\n",
      "         4 251400321         AHU    L2      AHU L2      AHU MDP 3.2 _OPMC_AHU L2.csv\n",
      "         5 251400321         AHU    L2      AHU L2      AHU MDP 3.2 _OPMC_AHU L2.csv\n",
      "\n",
      "🏢  MENGGABUNGKAN DATA OPMC + METER MAPPING\n",
      "🎉 Selesai OPMC: 3772 records, mapped 3772 (100.0%)\n",
      " record_id  meter_id system_type level description category               source_file\n",
      "         1 251400318         AHU    L3      AHU L3      AHU MDP 3.2 _OPMC_ AHU L3.csv\n",
      "         2 251400318         AHU    L3      AHU L3      AHU MDP 3.2 _OPMC_ AHU L3.csv\n",
      "         3 251400318         AHU    L3      AHU L3      AHU MDP 3.2 _OPMC_ AHU L3.csv\n",
      "         4 251400318         AHU    L3      AHU L3      AHU MDP 3.2 _OPMC_ AHU L3.csv\n",
      "         5 251400318         AHU    L3      AHU L3      AHU MDP 3.2 _OPMC_ AHU L3.csv\n",
      "🎉 Semua data gabungan disimpan: all_energy_data.csv\n"
     ]
    }
   ],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Merge WITEL & OPMC\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Merge Witel & OPMC\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1.  DICTIONARY MAPPING ID METER\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# WITEL (MDP 3.1)\n",
    "METER_MAP_WITEL = {\n",
    "    ('SDP', 'L1'): '251400383', ('SDP', 'L2'): '251400322', ('SDP', 'L3'): '251400320',\n",
    "    ('SDP', 'L4'): '251400277', ('SDP', 'L5'): '251400381', ('SDP', 'L6'): '251400384',\n",
    "    ('SDP', 'L7'): '251400279', ('SDP', 'L8'): '251400278',\n",
    "    ('AHU', 'L1'): '251400251', ('AHU', 'L2'): '251400321', ('AHU', 'L3'): '251400387',\n",
    "    ('AHU', 'L4'): '251400373', ('AHU', 'L5'): '251400316', ('AHU', 'L6'): '251400276',\n",
    "    ('AHU', 'L7'): '251400382', ('AHU', 'L8'): '251400385',\n",
    "    ('LIFT', 'General'): '251400184', ('CHILLER', 'General'): '251400177',\n",
    "}\n",
    "# OPMC (MDP 3.2)\n",
    "METER_MAP_OPMC = {\n",
    "    ('SDP', 'L1'): '251400282', ('SDP', 'L2'): '251400249', ('SDP', 'L3'): '251400248',\n",
    "    ('SDP', 'L4'): '251400319', ('SDP', 'L5'): '251400323', ('SDP', 'L6'): '251400386',\n",
    "    ('SDP', 'L7'): '251400279', ('SDP', 'L8'): '251400278',\n",
    "    ('AHU', 'L2'): '251400247', ('AHU', 'L3'): '251400318', ('AHU', 'L4'): '251400244',\n",
    "    ('AHU', 'L5'): '251400281', ('AHU', 'L6'): '251400270',\n",
    "    ('LIFT', 'General'): '251400378', ('DBT_AC', 'General'): '251400271',\n",
    "    ('SERVER_NOC', 'General'): '251400372', ('PANEL_RUANG_SISCA', 'L6'): '251400280',\n",
    "}\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2.  DETECTION & ASSIGNMENT FUNCTIONS\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def _detect_level(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Ekstrak level L1–L8 atau General.\n",
    "    \"\"\"\n",
    "    m = re.search(r'[Ll](\\d)', filename)\n",
    "    return f\"L{m.group(1)}\" if m else 'General'\n",
    "\n",
    "# System type for WITEL\n",
    "def _detect_system_type_witel(filename: str, category: str) -> str:\n",
    "    upper = filename.upper()\n",
    "    if 'CHILLER' in upper:\n",
    "        return 'CHILLER'\n",
    "    if 'LIFT' in upper:\n",
    "        return 'LIFT'\n",
    "    if 'RECTI' in upper:\n",
    "        return 'RECTIFIER'\n",
    "    if 'SERVER NOC' in upper or 'SERVER_NOC' in upper:\n",
    "        return 'SERVER_NOC'\n",
    "    return category\n",
    "\n",
    "# System type for OPMC\n",
    "def _detect_system_type_opmc(filename: str, category: str) -> str:\n",
    "    upper = filename.upper()\n",
    "    if 'DBT' in upper and 'AC' in upper:\n",
    "        return 'DBT_AC'\n",
    "    if 'SERVER NOC' in upper or 'SERVER_NOC' in upper:\n",
    "        return 'SERVER_NOC'\n",
    "    if 'SISCA' in upper:\n",
    "        return 'PANEL_RUANG_SISCA'\n",
    "    if 'LIFT' in upper:\n",
    "        return 'LIFT'\n",
    "    if 'CHILLER' in upper:\n",
    "        return 'CHILLER'\n",
    "    if 'RECTIFIER' in upper or 'RECTI' in upper:\n",
    "        return 'RECTIFIER'\n",
    "    return category\n",
    "\n",
    "# Assign meter id for either map\n",
    "def _assign_meter_id(meter_map: dict, system_type: str, level: str) -> tuple:\n",
    "    key = (system_type, level)\n",
    "    # fallback for utilities/general\n",
    "    if key not in meter_map and level == 'General':\n",
    "        key = (system_type, 'General')\n",
    "    meter_id = meter_map.get(key, 'Unknown')\n",
    "    description = f\"{system_type} {level}\"\n",
    "    return meter_id, description\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3.  GABUNG & MAPPING GENERIK\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def gabung_energy_data(building: str) -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Menggabungkan CSV untuk building ('WITEL'/'OPMC') dan menambahkan kolom meter_id.\n",
    "    \"\"\"\n",
    "    if building.upper() == 'WITEL':\n",
    "        patterns = {\n",
    "        # OPMC AHU L2–L6\n",
    "        'AHU':          \"project/data/raw/gedung_opmc/ahu/MDP 3.2 _OPMC_AHU L*.csv\",\n",
    "        # OPMC SDP L1–L6\n",
    "        'SDP':          \"project/data/raw/gedung_opmc/sdp/MDP 3.2 _OPMC_SDP L*.csv\",\n",
    "        # OPMC Utilities umum\n",
    "        'UTILITIES':    \"project/data/raw/gedung_opmc/utilities/*.csv\",\n",
    "        # OPMC DBT AC dan Panel Khusus\n",
    "        'DBT':          \"project/data/raw/gedung_opmc/dbt/MDP 2_DBT AC.csv\",\n",
    "        'PANEL_KHUSUS': \"project/data/raw/panel_other/MDP 1_*.csv\",\n",
    "    }\n",
    "        meter_map = METER_MAP_WITEL\n",
    "        detect_type = _detect_system_type_witel\n",
    "        out_file = 'witel_energy_data.csv'\n",
    "        title = 'WITEL'\n",
    "    elif building.upper() == 'OPMC':\n",
    "        patterns = {'AHU': \"project/data/raw/gedung_opmc/ahu/*.csv\",\n",
    "                    'SDP': \"project/data/raw/gedung_opmc/sdp/*.csv\",\n",
    "                    'UTILITIES': \"project/data/raw/gedung_opmc/utilities/*.csv\",\n",
    "                    'DBT': \"project/data/raw/gedung_opmc/dbt/*.csv\",\n",
    "                    'PANEL_KHUSUS': \"project/data/raw/gedung_opmc/panel_khusus/*.csv\"}\n",
    "        meter_map = METER_MAP_OPMC\n",
    "        detect_type = _detect_system_type_opmc\n",
    "        out_file = 'opmc_energy_data.csv'\n",
    "        title = 'OPMC'\n",
    "    else:\n",
    "        raise ValueError(\"Building harus 'WITEL' atau 'OPMC'.\")\n",
    "\n",
    "    all_df, summary = [], {}\n",
    "    print(f\"\\n🏢  MENGGABUNGKAN DATA {title} + METER MAPPING\")\n",
    "    for cat, pat in patterns.items():\n",
    "        files = glob.glob(pat, recursive=True)\n",
    "        summary[cat] = 0\n",
    "        for path in files:\n",
    "            try:\n",
    "                df = pd.read_csv(path)\n",
    "                fname = os.path.basename(path)\n",
    "                level = _detect_level(fname)\n",
    "                s_type = detect_type(fname, cat)\n",
    "                meter_id, m_desc = _assign_meter_id(meter_map, s_type, level)\n",
    "                # metadata\n",
    "                df['building'] = title\n",
    "                df['category'] = cat\n",
    "                df['system_type'] = s_type\n",
    "                df['level'] = level\n",
    "                df['meter_id'] = meter_id\n",
    "                df['description'] = m_desc\n",
    "                df['source_file'] = fname\n",
    "                df['file_path'] = path\n",
    "                df['processed_ts'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                all_df.append(df)\n",
    "                summary[cat] += len(df)\n",
    "            except Exception as e:\n",
    "                print(f\"❌ {fname}: {e}\")\n",
    "    if not all_df:\n",
    "        print(\"❌ Tidak ada data!\")\n",
    "        return None\n",
    "    combined = pd.concat(all_df, ignore_index=True)\n",
    "    combined.insert(0, 'record_id', range(1, len(combined)+1))\n",
    "    combined.to_csv(out_file, index=False, encoding='utf-8')\n",
    "    # ringkasan\n",
    "    total = len(combined)\n",
    "    mapped = (combined['meter_id'] != 'Unknown').sum()\n",
    "    print(f\"🎉 Selesai {title}: {total} records, mapped {mapped} ({mapped/total:.1%})\")\n",
    "    return combined\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 4.  PREVIEW\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def preview(data: pd.DataFrame, n: int = 5):\n",
    "    if data is None:\n",
    "        return\n",
    "    cols = ['record_id', 'meter_id', 'system_type', 'level', 'description', 'category', 'source_file']\n",
    "    print(data[cols].head(n).to_string(index=False))\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 5.  EKSEKUSI UTAMA\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "if __name__ == '__main__':\n",
    "    df_witel = gabung_energy_data('WITEL')\n",
    "    preview(df_witel)\n",
    "    df_opmc = gabung_energy_data('OPMC')\n",
    "    preview(df_opmc)\n",
    "    # gabungkan kedua output jika diperlukan\n",
    "    if df_witel is not None and df_opmc is not None:\n",
    "        df_all = pd.concat([df_witel, df_opmc], ignore_index=True)\n",
    "        df_all.to_csv('all_energy_data.csv', index=False)\n",
    "        print(\"🎉 Semua data gabungan disimpan: all_energy_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "33471bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏢  MENGGABUNGKAN DATA WITEL ENERGY + METER MAPPING\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "📂 AHU       : 8 file\n",
      "  ✅ MDP 3. 1_Wittel_AHU L1.csv               |    999 rows | meter_id: 251400251\n",
      "  ✅ MDP 3. 1_Wittel_AHU L2.csv               |    344 rows | meter_id: 251400321\n",
      "  ✅ MDP 3. 1_Wittel_AHU L3.csv               |    344 rows | meter_id: 251400387\n",
      "  ✅ MDP 3. 1_Wittel_AHU L4.csv               |    345 rows | meter_id: 251400373\n",
      "  ✅ MDP 3. 1_Wittel_AHU L5.csv               |    344 rows | meter_id: 251400316\n",
      "  ✅ MDP 3. 1_Wittel_AHU L6.csv               |    345 rows | meter_id: 251400276\n",
      "  ✅ MDP 3. 1_Wittel_AHU L7.csv               |    343 rows | meter_id: 251400382\n",
      "  ✅ MDP 3. 1_Wittel_AHU L8.csv               |    344 rows | meter_id: 251400385\n",
      "  📊 TOTAL AHU       : 3,408 rows\n",
      "\n",
      "📂 SDP       : 9 file\n",
      "  ✅ MDP 3. 1_Wittel_SDP L1.csv               |    345 rows | meter_id: 251400383\n",
      "  ✅ MDP 3. 1_Wittel_SDP L2.csv               |    339 rows | meter_id: 251400322\n",
      "  ✅ MDP 3. 1_Wittel_SDP L3.csv               |    341 rows | meter_id: 251400320\n",
      "  ✅ MDP 3. 1_Wittel_SDP L4.csv               |    342 rows | meter_id: 251400277\n",
      "  ✅ MDP 3. 1_Wittel_SDP L5.csv               |    341 rows | meter_id: 251400381\n",
      "  ✅ MDP 3. 1_Wittel_SDP L6.csv               |    338 rows | meter_id: 251400384\n",
      "  ✅ MDP 3. 1_Wittel_SDP L6_SERVER NOC.csv    |    341 rows | meter_id: Unknown\n",
      "  ✅ MDP 3. 1_Wittel_SDP L7.csv               |    341 rows | meter_id: 251400279\n",
      "  ✅ MDP 3. 1_Wittel_SDP L8.csv               |    344 rows | meter_id: 251400278\n",
      "  📊 TOTAL SDP       : 3,072 rows\n",
      "\n",
      "📂 UTILITIES : 3 file\n",
      "  ✅ MDP 3. 1_Wittel_CHILLER.csv              |     47 rows | meter_id: 251400177\n",
      "  ✅ MDP 3. 1_Wittel_LIFT.csv                 |    343 rows | meter_id: 251400184\n",
      "  ✅ MDP 3. 1_Wittel_SDP RECTI.csv            |    339 rows | meter_id: Unknown\n",
      "  📊 TOTAL UTILITIES : 729 rows\n",
      "\n",
      "🎉  PENGGABUNGAN SELESAI\n",
      "──────────────────────────────────────────────────\n",
      "📊  Total records  : 7,209\n",
      "   • AHU       : 3,408 rows\n",
      "   • SDP       : 3,072 rows\n",
      "   • UTILITIES : 729 rows\n",
      "✅  Meter mapped   : 6,529  (90.6%)\n",
      "📁  Output         : witel_energy_data.csv\n",
      "\n",
      "👀  PREVIEW\n",
      " record_id  meter_id system_type level description category                source_file\n",
      "         1 251400251         AHU    L1      AHU L1      AHU MDP 3. 1_Wittel_AHU L1.csv\n",
      "         2 251400251         AHU    L1      AHU L1      AHU MDP 3. 1_Wittel_AHU L1.csv\n",
      "         3 251400251         AHU    L1      AHU L1      AHU MDP 3. 1_Wittel_AHU L1.csv\n",
      "         4 251400251         AHU    L1      AHU L1      AHU MDP 3. 1_Wittel_AHU L1.csv\n",
      "         5 251400251         AHU    L1      AHU L1      AHU MDP 3. 1_Wittel_AHU L1.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1.  DICTIONARY MAPPING ID METER  (WITEL MDP 3.1)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "METER_MAP = {\n",
    "    # SDP\n",
    "    ('SDP', 'L1'): '251400383',\n",
    "    ('SDP', 'L2'): '251400322',\n",
    "    ('SDP', 'L3'): '251400320',\n",
    "    ('SDP', 'L4'): '251400277',\n",
    "    ('SDP', 'L5'): '251400381',\n",
    "    ('SDP', 'L6'): '251400384',\n",
    "    ('SDP', 'L7'): '251400279',\n",
    "    ('SDP', 'L8'): '251400278',\n",
    "    # AHU\n",
    "    ('AHU', 'L1'): '251400251',\n",
    "    ('AHU', 'L2'): '251400321',\n",
    "    ('AHU', 'L3'): '251400387',\n",
    "    ('AHU', 'L4'): '251400373',\n",
    "    ('AHU', 'L5'): '251400316',\n",
    "    ('AHU', 'L6'): '251400276',\n",
    "    ('AHU', 'L7'): '251400382',\n",
    "    ('AHU', 'L8'): '251400385',\n",
    "    # UTILITIES\n",
    "    ('LIFT',     'General'): '251400184',\n",
    "    ('CHILLER',  'General'): '251400177',\n",
    "}\n",
    "\n",
    "def _detect_level(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Ekstrak L1 … L8 dari nama file; jika tidak ada, kembalikan 'General'\n",
    "    \"\"\"\n",
    "    m = re.search(r'[Ll](\\d)', filename)\n",
    "    return f\"L{m.group(1)}\" if m else 'General'\n",
    "\n",
    "def _detect_system_type(filename: str, category: str) -> str:\n",
    "    \"\"\"\n",
    "    Tentukan system_type yang lebih spesifik daripada sekadar kategori folder\n",
    "    \"\"\"\n",
    "    upper_name = filename.upper()\n",
    "    if 'CHILLER' in upper_name:\n",
    "        return 'CHILLER'\n",
    "    if 'LIFT' in upper_name:\n",
    "        return 'LIFT'\n",
    "    if 'RECTI' in upper_name:\n",
    "        return 'RECTIFIER'\n",
    "    if 'SERVER NOC' in upper_name or 'SERVER_NOC' in upper_name:\n",
    "        return 'SERVER_NOC'\n",
    "    return category          # fallback: AHU / SDP / UTILITIES\n",
    "\n",
    "def _assign_meter_id(system_type: str, level: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Kembalikan (meter_id, mapped_description) sesuai tabel METER_MAP\n",
    "    \"\"\"\n",
    "    key = (system_type, level)\n",
    "    # UTILITIES pakai 'General' sebagai level\n",
    "    if key not in METER_MAP and system_type in ('LIFT', 'CHILLER'):\n",
    "        key = (system_type, 'General')\n",
    "    meter_id = METER_MAP.get(key, 'Unknown')\n",
    "    description = f\"{system_type} {level}\"\n",
    "    return meter_id, description\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2.  FUNGSI PENGGABUNGAN + MAPPING\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def gabung_witel_energy_data():\n",
    "    \"\"\"\n",
    "    Menggabungkan semua CSV WITEL dan menambahkan kolom meter_id sesuai diagram.\n",
    "    \"\"\"\n",
    "    patterns = {\n",
    "        'AHU':        \"project/data/raw/gedung_witel/ahu/*.csv\",\n",
    "        'SDP':        \"project/data/raw/gedung_witel/sdp/*.csv\",\n",
    "        'UTILITIES':  \"project/data/raw/gedung_witel/utilities/*.csv\",\n",
    "    }\n",
    "\n",
    "    all_df, summary = [], {}\n",
    "    print(\"\\n🏢  MENGGABUNGKAN DATA WITEL ENERGY + METER MAPPING\")\n",
    "    print(\"──────────────────────────────────────────────────\")\n",
    "\n",
    "    for cat, pattern in patterns.items():\n",
    "        files = glob.glob(pattern, recursive=True)\n",
    "        if not files:\n",
    "            print(f\"⚠️  {cat:<10}: 0 file\")\n",
    "            summary[cat] = 0\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n📂 {cat:<10}: {len(files)} file\")\n",
    "        total_rows = 0\n",
    "\n",
    "        for path in files:\n",
    "            try:\n",
    "                df = pd.read_csv(path)\n",
    "                fname  = os.path.basename(path)\n",
    "                level  = _detect_level(fname)\n",
    "                s_type = _detect_system_type(fname, cat)\n",
    "                meter_id, m_desc = _assign_meter_id(s_type, level)\n",
    "\n",
    "                # tambah metadata\n",
    "                df['building']    = 'WITEL'\n",
    "                df['category']    = cat\n",
    "                df['system_type'] = s_type\n",
    "                df['level']       = level\n",
    "                df['meter_id']    = meter_id\n",
    "                df['description'] = m_desc\n",
    "                df['source_file'] = fname\n",
    "                df['file_path']   = path\n",
    "                df['processed_ts'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                all_df.append(df)\n",
    "                total_rows += len(df)\n",
    "                print(f\"  ✅ {fname:<40} | {len(df):>6} rows | meter_id: {meter_id}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ {os.path.basename(path)}: {e}\")\n",
    "\n",
    "        summary[cat] = total_rows\n",
    "        print(f\"  📊 TOTAL {cat:<10}: {total_rows:,} rows\")\n",
    "\n",
    "    if not all_df:\n",
    "        print(\"\\n❌ Tidak ada data yang berhasil digabungkan!\")\n",
    "        return None\n",
    "\n",
    "    # Gabung & simpan\n",
    "    combined = pd.concat(all_df, ignore_index=True)\n",
    "    combined.insert(0, 'record_id', range(1, len(combined)+1))\n",
    "    out_file = 'witel_energy_data.csv'\n",
    "    combined.to_csv(out_file, index=False, encoding='utf-8')\n",
    "\n",
    "    # ── RINGKASAN ─────────────────────────────────────\n",
    "    print(\"\\n🎉  PENGGABUNGAN SELESAI\")\n",
    "    print(\"──────────────────────────────────────────────────\")\n",
    "    print(f\"📊  Total records  : {len(combined):,}\")\n",
    "    for cat in ('AHU', 'SDP', 'UTILITIES'):\n",
    "        print(f\"   • {cat:<10}: {summary.get(cat,0):,} rows\")\n",
    "    mapped_ok = (combined['meter_id'] != 'Unknown').sum()\n",
    "    print(f\"✅  Meter mapped   : {mapped_ok:,}  ({mapped_ok/len(combined)*100:.1f}%)\")\n",
    "    print(f\"📁  Output         : {out_file}\")\n",
    "\n",
    "    return combined\n",
    "\n",
    "def preview(data: pd.DataFrame, n: int = 5):\n",
    "    if data is None: return\n",
    "    show_cols = ['record_id', 'meter_id', 'system_type', 'level',\n",
    "                 'description', 'category', 'source_file']\n",
    "    print(\"\\n👀  PREVIEW\")\n",
    "    print(data[show_cols].head(n).to_string(index=False))\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3.  EKSEKUSI\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    df_witel = gabung_witel_energy_data()\n",
    "    preview(df_witel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d2b527d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏢  MENGGABUNGKAN DATA OPMC ENERGY + METER MAPPING\n",
      "──────────────────────────────────────────────────\n",
      "\n",
      "📂 AHU            : 5 file\n",
      "  ✅ MDP 3.2 _OPMC_ AHU L3.csv                     |    338 rows | meter_id: 251400318\n",
      "  ✅ MDP 3.2 _OPMC_AHU L2.csv                      |    342 rows | meter_id: 251400247\n",
      "  ✅ MDP 3.2 _OPMC_AHU L4.csv                      |    344 rows | meter_id: 251400244\n",
      "  ✅ MDP 3.2 _OPMC_AHU L5.csv                      |    342 rows | meter_id: 251400281\n",
      "  ✅ MDP 3.2 _OPMC_AHU L6.csv                      |    344 rows | meter_id: 251400270\n",
      "  📊 TOTAL AHU            : 1,710 rows\n",
      "\n",
      "📂 SDP            : 6 file\n",
      "  ✅ MDP 3.2 _OPMC_ SDP L3.csv                     |    344 rows | meter_id: 251400248\n",
      "  ✅ MDP 3.2 _OPMC_SDP L1.csv                      |    345 rows | meter_id: 251400282\n",
      "  ✅ MDP 3.2 _OPMC_SDP L2.csv                      |    343 rows | meter_id: 251400249\n",
      "  ✅ MDP 3.2 _OPMC_SDP L4.csv                      |    345 rows | meter_id: 251400319\n",
      "  ✅ MDP 3.2 _OPMC_SDP L5.csv                      |    342 rows | meter_id: 251400323\n",
      "  ✅ MDP 3.2 _OPMC_SDP L6.csv                      |    343 rows | meter_id: 251400386\n",
      "  📊 TOTAL SDP            : 2,062 rows\n",
      "⚠️  UTILITIES      : 0 file\n",
      "⚠️  DBT            : 0 file\n",
      "⚠️  PANEL_KHUSUS   : 0 file\n",
      "\n",
      "🎉  PENGGABUNGAN SELESAI\n",
      "──────────────────────────────────────────────────\n",
      "📊  Total records  : 3,772\n",
      "   • AHU            : 1,710 rows\n",
      "   • SDP            : 2,062 rows\n",
      "   • UTILITIES      : 0 rows\n",
      "   • DBT            : 0 rows\n",
      "   • PANEL_KHUSUS   : 0 rows\n",
      "✅  Meter mapped   : 3,772  (100.0%)\n",
      "📁  Output         : opmc_energy_data.csv\n",
      "\n",
      "📋  METER ID SUMMARY:\n",
      "   • AHU             L2         -> 251400247 (342 records)\n",
      "   • AHU             L3         -> 251400318 (338 records)\n",
      "   • AHU             L4         -> 251400244 (344 records)\n",
      "   • AHU             L5         -> 251400281 (342 records)\n",
      "   • AHU             L6         -> 251400270 (344 records)\n",
      "   • SDP             L1         -> 251400282 (345 records)\n",
      "   • SDP             L2         -> 251400249 (343 records)\n",
      "   • SDP             L3         -> 251400248 (344 records)\n",
      "   • SDP             L4         -> 251400319 (345 records)\n",
      "   • SDP             L5         -> 251400323 (342 records)\n",
      "   • SDP             L6         -> 251400386 (343 records)\n",
      "\n",
      "👀  PREVIEW\n",
      " record_id  meter_id system_type level description category               source_file\n",
      "         1 251400318         AHU    L3      AHU L3      AHU MDP 3.2 _OPMC_ AHU L3.csv\n",
      "         2 251400318         AHU    L3      AHU L3      AHU MDP 3.2 _OPMC_ AHU L3.csv\n",
      "         3 251400318         AHU    L3      AHU L3      AHU MDP 3.2 _OPMC_ AHU L3.csv\n",
      "         4 251400318         AHU    L3      AHU L3      AHU MDP 3.2 _OPMC_ AHU L3.csv\n",
      "         5 251400318         AHU    L3      AHU L3      AHU MDP 3.2 _OPMC_ AHU L3.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1.  DICTIONARY MAPPING ID METER  (OPMC MDP 3.2) - LENGKAP DARI DIAGRAM\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "METER_MAP = {\n",
    "    # SDP panel utama OPMC\n",
    "    ('SDP', 'L1'): '251400282',\n",
    "    ('SDP', 'L2'): '251400249', \n",
    "    ('SDP', 'L3'): '251400248',\n",
    "    ('SDP', 'L4'): '251400319',\n",
    "    ('SDP', 'L5'): '251400323',\n",
    "    ('SDP', 'L6'): '251400386',\n",
    "    \n",
    "    # AHU panel OPMC (L2-L6 sesuai diagram)\n",
    "    ('AHU', 'L2'): '251400247',\n",
    "    ('AHU', 'L3'): '251400318', \n",
    "    ('AHU', 'L4'): '251400244',\n",
    "    ('AHU', 'L5'): '251400281',\n",
    "    ('AHU', 'L6'): '251400270',\n",
    "    \n",
    "    # Panel utility dan khusus\n",
    "    ('LIFT', 'General'): '251400378',\n",
    "    ('DBT_AC', 'General'): '251400271',\n",
    "    ('SERVER_NOC', 'General'): '251400372',\n",
    "    ('PANEL_RUANG_SISCA', 'L6'): '251400280',\n",
    "    \n",
    "    # Panel-panel dari diagram WITEL (SDP L6-L8, masih dalam sistem)\n",
    "    ('SDP', 'L7'): '251400279',\n",
    "    ('SDP', 'L8'): '251400278',\n",
    "}\n",
    "\n",
    "def _detect_level(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Ekstrak level dari nama file dengan berbagai pola\n",
    "    \"\"\"\n",
    "    upper_name = filename.upper()\n",
    "    \n",
    "    # Deteksi level L1-L8\n",
    "    m = re.search(r'[Ll](\\d)', filename)\n",
    "    if m:\n",
    "        return f\"L{m.group(1)}\"\n",
    "    \n",
    "    # Deteksi pola khusus\n",
    "    if 'SISCA' in upper_name and any(x in upper_name for x in ['L6', 'LANTAI 6']):\n",
    "        return 'L6'\n",
    "    \n",
    "    return 'General'\n",
    "\n",
    "def _detect_system_type(filename: str, category: str) -> str:\n",
    "    \"\"\"\n",
    "    Tentukan system_type berdasarkan nama file dan kategori\n",
    "    \"\"\"\n",
    "    upper_name = filename.upper()\n",
    "    \n",
    "    # Deteksi sistem khusus\n",
    "    if 'DBT' in upper_name and 'AC' in upper_name:\n",
    "        return 'DBT_AC'\n",
    "    if 'SERVER NOC' in upper_name or 'SERVER_NOC' in upper_name:\n",
    "        return 'SERVER_NOC'\n",
    "    if 'SISCA' in upper_name:\n",
    "        return 'PANEL_RUANG_SISCA'\n",
    "    if 'LIFT' in upper_name:\n",
    "        return 'LIFT'\n",
    "    if 'CHILLER' in upper_name:\n",
    "        return 'CHILLER'\n",
    "    if 'RECTIFIER' in upper_name or 'RECTI' in upper_name:\n",
    "        return 'RECTIFIER'\n",
    "    \n",
    "    # Fallback ke kategori folder\n",
    "    return category\n",
    "\n",
    "def _assign_meter_id(system_type: str, level: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Kembalikan (meter_id, mapped_description) sesuai tabel METER_MAP\n",
    "    \"\"\"\n",
    "    key = (system_type, level)\n",
    "    \n",
    "    # Untuk panel utility dengan level General\n",
    "    if key not in METER_MAP and system_type in ('LIFT', 'DBT_AC', 'SERVER_NOC'):\n",
    "        key = (system_type, 'General')\n",
    "    \n",
    "    meter_id = METER_MAP.get(key, 'Unknown')\n",
    "    description = f\"{system_type} {level}\"\n",
    "    return meter_id, description\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2.  FUNGSI PENGGABUNGAN + MAPPING\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def gabung_opmc_energy_data():\n",
    "    \"\"\"\n",
    "    Menggabungkan semua CSV OPMC dan menambahkan kolom meter_id sesuai diagram.\n",
    "    \"\"\"\n",
    "    patterns = {\n",
    "        'AHU':        \"project/data/raw/gedung_opmc/ahu/*.csv\",\n",
    "        'SDP':        \"project/data/raw/gedung_opmc/sdp/*.csv\",\n",
    "        'UTILITIES':  \"project/data/raw/gedung_opmc/utilities/*.csv\",\n",
    "        'DBT':        \"project/data/raw/gedung_opmc/dbt/*.csv\",\n",
    "        'PANEL_KHUSUS': \"project/data/raw/gedung_opmc/panel_khusus/*.csv\",\n",
    "    }\n",
    "\n",
    "    all_df, summary = [], {}\n",
    "    print(\"\\n🏢  MENGGABUNGKAN DATA OPMC ENERGY + METER MAPPING\")\n",
    "    print(\"──────────────────────────────────────────────────\")\n",
    "\n",
    "    for cat, pattern in patterns.items():\n",
    "        files = glob.glob(pattern, recursive=True)\n",
    "        if not files:\n",
    "            print(f\"⚠️  {cat:<15}: 0 file\")\n",
    "            summary[cat] = 0\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n📂 {cat:<15}: {len(files)} file\")\n",
    "        total_rows = 0\n",
    "\n",
    "        for path in files:\n",
    "            try:\n",
    "                df = pd.read_csv(path)\n",
    "                fname  = os.path.basename(path)\n",
    "                level  = _detect_level(fname)\n",
    "                s_type = _detect_system_type(fname, cat)\n",
    "                meter_id, m_desc = _assign_meter_id(s_type, level)\n",
    "\n",
    "                # tambah metadata\n",
    "                df['building']     = 'OPMC'\n",
    "                df['category']     = cat\n",
    "                df['system_type']  = s_type\n",
    "                df['level']        = level\n",
    "                df['meter_id']     = meter_id\n",
    "                df['description']  = m_desc\n",
    "                df['source_file']  = fname\n",
    "                df['file_path']    = path\n",
    "                df['processed_ts'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                all_df.append(df)\n",
    "                total_rows += len(df)\n",
    "                print(f\"  ✅ {fname:<45} | {len(df):>6} rows | meter_id: {meter_id}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ {os.path.basename(path)}: {e}\")\n",
    "\n",
    "        summary[cat] = total_rows\n",
    "        print(f\"  📊 TOTAL {cat:<15}: {total_rows:,} rows\")\n",
    "\n",
    "    if not all_df:\n",
    "        print(\"\\n❌ Tidak ada data yang berhasil digabungkan!\")\n",
    "        return None\n",
    "\n",
    "    # Gabung & simpan\n",
    "    combined = pd.concat(all_df, ignore_index=True)\n",
    "    combined.insert(0, 'record_id', range(1, len(combined)+1))\n",
    "    out_file = 'opmc_energy_data.csv'\n",
    "    combined.to_csv(out_file, index=False, encoding='utf-8')\n",
    "\n",
    "    # ── RINGKASAN ─────────────────────────────────────\n",
    "    print(\"\\n🎉  PENGGABUNGAN SELESAI\")\n",
    "    print(\"──────────────────────────────────────────────────\")\n",
    "    print(f\"📊  Total records  : {len(combined):,}\")\n",
    "    for cat in patterns.keys():\n",
    "        print(f\"   • {cat:<15}: {summary.get(cat,0):,} rows\")\n",
    "    mapped_ok = (combined['meter_id'] != 'Unknown').sum()\n",
    "    print(f\"✅  Meter mapped   : {mapped_ok:,}  ({mapped_ok/len(combined)*100:.1f}%)\")\n",
    "    print(f\"📁  Output         : {out_file}\")\n",
    "\n",
    "    # Tampilkan ringkasan meter mapping\n",
    "    print(\"\\n📋  METER ID SUMMARY:\")\n",
    "    meter_summary = combined.groupby(['system_type', 'level', 'meter_id']).size().reset_index(name='count')\n",
    "    for _, row in meter_summary.iterrows():\n",
    "        print(f\"   • {row['system_type']:<15} {row['level']:<10} -> {row['meter_id']} ({row['count']} records)\")\n",
    "\n",
    "    return combined\n",
    "\n",
    "def preview(data: pd.DataFrame, n: int = 5):\n",
    "    if data is None: return\n",
    "    show_cols = ['record_id', 'meter_id', 'system_type', 'level',\n",
    "                 'description', 'category', 'source_file']\n",
    "    print(\"\\n👀  PREVIEW\")\n",
    "    print(data[show_cols].head(n).to_string(index=False))\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3.  EKSEKUSI\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    df_opmc = gabung_opmc_energy_data()\n",
    "    preview(df_opmc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae6cf1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌟 STARTING OCCUPANCY PROCESSING - ENERGY DATA ALIGNED\n",
      "================================================================================\n",
      "📋 Requirements:\n",
      "   1. ✅ Building classification: WITEL & OPMC\n",
      "   2. ✅ Meter ID mengikuti pola energy meter\n",
      "   3. ✅ Level: General (tidak ada L1-L8)\n",
      "   4. ✅ Date alignment dengan energy data\n",
      "   5. ✅ Agregasi harian seperti energy data\n",
      "================================================================================\n",
      "🚀 MEMPROSES OCCUPANCY UNTUK WITEL & OPMC\n",
      "======================================================================\n",
      "\n",
      "📅 MENGECEK RENTANG TANGGAL ENERGY DATA:\n",
      "  ✅ Energy data date range: 2024-01-01 to 2024-12-31\n",
      "\n",
      "🏢 PROCESSING WITEL:\n",
      "----------------------------------------\n",
      "🏢 MEMPROSES DATA OKUPANSI - WITEL\n",
      "==================================================\n",
      "✅ Berhasil membaca file: 82880 baris data\n",
      "\n",
      "📊 INFO DATA AWAL:\n",
      "Building Type: WITEL\n",
      "Kolom tersedia: ['timestamp', 'Lantai_No', 'Ruangan', 'Occupansi']\n",
      "Shape data: (82880, 4)\n",
      "\n",
      "👀 PREVIEW DATA (5 baris pertama):\n",
      "    timestamp  Lantai_No           Ruangan  Occupansi\n",
      "0  01/01/2024          1            Satpam          4\n",
      "1  01/01/2024          2      Gen Y Corner          8\n",
      "2  01/01/2024          2           Warroom          2\n",
      "3  01/01/2024          2         Fosikatel          2\n",
      "4  01/01/2024          2  Business service          7\n",
      "\n",
      "🔍 MENGIDENTIFIKASI KOLOM DATETIME:\n",
      "  📅 Kandidat datetime: timestamp\n",
      "✅ Menggunakan kolom datetime: timestamp\n",
      "\n",
      "⏰ MENGKONVERSI KOLOM timestamp KE DATETIME:\n",
      "  ✅ Berhasil dengan format: %d/%m/%Y\n",
      "\n",
      "📈 MELAKUKAN AGREGASI HARIAN (ENERGY DATA ALIGNED - FIXED):\n",
      "  📅 Filtered to energy date range: 2024-01-01 to 2024-12-31\n",
      "  📊 Records after filtering: 82880\n",
      "  📊 Kolom okupansi yang akan diagregasi: ['Lantai_No', 'Occupansi']\n",
      "  🏗️  Menambahkan metadata struktur energy data...\n",
      "  ✅ Berhasil agregasi: 366 hari data\n",
      "  📊 Kolom hasil agregasi: 21 kolom\n",
      "💾 Data WITEL disimpan ke: witel_occupancy_daily.csv\n",
      "\n",
      "🏢 PROCESSING OPMC:\n",
      "----------------------------------------\n",
      "🏢 MEMPROSES DATA OKUPANSI - OPMC\n",
      "==================================================\n",
      "✅ Berhasil membaca file: 82880 baris data\n",
      "\n",
      "📊 INFO DATA AWAL:\n",
      "Building Type: OPMC\n",
      "Kolom tersedia: ['timestamp', 'Lantai_No', 'Ruangan', 'Occupansi']\n",
      "Shape data: (82880, 4)\n",
      "\n",
      "👀 PREVIEW DATA (5 baris pertama):\n",
      "    timestamp  Lantai_No           Ruangan  Occupansi\n",
      "0  01/01/2024          1            Satpam          4\n",
      "1  01/01/2024          2      Gen Y Corner          8\n",
      "2  01/01/2024          2           Warroom          2\n",
      "3  01/01/2024          2         Fosikatel          2\n",
      "4  01/01/2024          2  Business service          7\n",
      "\n",
      "🔍 MENGIDENTIFIKASI KOLOM DATETIME:\n",
      "  📅 Kandidat datetime: timestamp\n",
      "✅ Menggunakan kolom datetime: timestamp\n",
      "\n",
      "⏰ MENGKONVERSI KOLOM timestamp KE DATETIME:\n",
      "  ✅ Berhasil dengan format: %d/%m/%Y\n",
      "\n",
      "📈 MELAKUKAN AGREGASI HARIAN (ENERGY DATA ALIGNED - FIXED):\n",
      "  📅 Filtered to energy date range: 2024-01-01 to 2024-12-31\n",
      "  📊 Records after filtering: 82880\n",
      "  📊 Kolom okupansi yang akan diagregasi: ['Lantai_No', 'Occupansi']\n",
      "  🏗️  Menambahkan metadata struktur energy data...\n",
      "  ✅ Berhasil agregasi: 366 hari data\n",
      "  📊 Kolom hasil agregasi: 21 kolom\n",
      "💾 Data OPMC disimpan ke: opmc_occupancy_daily.csv\n",
      "\n",
      "🔗 MENGGABUNGKAN OCCUPANCY DATA:\n",
      "✅ Data gabungan: 732 records\n",
      "💾 Disimpan ke: all_occupancy_data.csv\n",
      "\n",
      "👀 PREVIEW HASIL GABUNGAN:\n",
      "====================================================================================================\n",
      " record_id       date building  meter_id      system_type   level                                  description  category  Lantai_No_mean  Occupansi_mean\n",
      "         1 2024-01-01    WITEL 251400901 OCCUPANCY_SENSOR General WITEL OCCUPANCY_SENSOR Daily Aggregated Data OCCUPANCY            4.78            7.59\n",
      "         2 2024-01-02    WITEL 251400901 OCCUPANCY_SENSOR General WITEL OCCUPANCY_SENSOR Daily Aggregated Data OCCUPANCY            4.76            7.41\n",
      "         3 2024-01-03    WITEL 251400901 OCCUPANCY_SENSOR General WITEL OCCUPANCY_SENSOR Daily Aggregated Data OCCUPANCY            4.78            7.28\n",
      "         4 2024-01-04    WITEL 251400901 OCCUPANCY_SENSOR General WITEL OCCUPANCY_SENSOR Daily Aggregated Data OCCUPANCY            4.77            7.90\n",
      "         5 2024-01-05    WITEL 251400901 OCCUPANCY_SENSOR General WITEL OCCUPANCY_SENSOR Daily Aggregated Data OCCUPANCY            4.77            7.45\n",
      "         6 2024-01-06    WITEL 251400901 OCCUPANCY_SENSOR General WITEL OCCUPANCY_SENSOR Daily Aggregated Data OCCUPANCY            4.78            7.37\n",
      "         7 2024-01-07    WITEL 251400901 OCCUPANCY_SENSOR General WITEL OCCUPANCY_SENSOR Daily Aggregated Data OCCUPANCY            4.82            7.99\n",
      "         8 2024-01-08    WITEL 251400901 OCCUPANCY_SENSOR General WITEL OCCUPANCY_SENSOR Daily Aggregated Data OCCUPANCY            4.76            7.52\n",
      "         9 2024-01-09    WITEL 251400901 OCCUPANCY_SENSOR General WITEL OCCUPANCY_SENSOR Daily Aggregated Data OCCUPANCY            4.78            7.53\n",
      "        10 2024-01-10    WITEL 251400901 OCCUPANCY_SENSOR General WITEL OCCUPANCY_SENSOR Daily Aggregated Data OCCUPANCY            4.77            7.23\n",
      "\n",
      "🔄 MENGINTEGRASIKAN DENGAN ENERGY DATA:\n",
      "============================================================\n",
      "📊 Occupancy records: 732\n",
      "⚡ Energy records: 7207\n",
      "✅ Data terintegrasi: 7939 records\n",
      "💾 Disimpan ke: all_building_integrated_data.csv\n",
      "\n",
      "📈 RINGKASAN INTEGRASI:\n",
      "building     category  count\n",
      "    OPMC          AHU   1710\n",
      "    OPMC    OCCUPANCY    366\n",
      "    OPMC          SDP   2062\n",
      "   WITEL          AHU   1372\n",
      "   WITEL    OCCUPANCY    366\n",
      "   WITEL PANEL_KHUSUS    345\n",
      "   WITEL          SDP   1718\n",
      "\n",
      "🎉 PROSES SELESAI!\n",
      "==================================================\n",
      "📁 Files yang dihasilkan:\n",
      "   • witel_occupancy_daily.csv\n",
      "   • opmc_occupancy_daily.csv\n",
      "   • all_occupancy_data.csv\n",
      "   • all_building_integrated_data.csv\n",
      "\n",
      "✨ Occupancy data sudah selaras dengan energy data structure!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1. METER ID MAPPING UNTUK OCCUPANCY (MENGIKUTI POLA ENERGY)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Synthetic Meter IDs untuk Occupancy (mengikuti pola energy meter)\n",
    "METER_MAP_OCCUPANCY = {\n",
    "    'WITEL': {\n",
    "        ('OCCUPANCY_SENSOR', 'General'): '251400901',\n",
    "        ('OCCUPANCY_COUNTER', 'General'): '251400902',\n",
    "        ('PEOPLE_COUNT', 'General'): '251400903',\n",
    "    },\n",
    "    'OPMC': {\n",
    "        ('OCCUPANCY_SENSOR', 'General'): '251400904',\n",
    "        ('OCCUPANCY_COUNTER', 'General'): '251400905',\n",
    "        ('PEOPLE_COUNT', 'General'): '251400906',\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_occupancy_meter_id(building, system_type='OCCUPANCY_SENSOR'):\n",
    "    \"\"\"\n",
    "    Mendapatkan meter ID untuk occupancy berdasarkan building dan system type\n",
    "    \"\"\"\n",
    "    key = (system_type, 'General')\n",
    "    return METER_MAP_OCCUPANCY.get(building, {}).get(key, f'OCC_{building}_001')\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2. FUNGSI BACA DATA DENGAN BUILDING CLASSIFICATION\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def proses_agregasi_occupancy_enhanced(file_path=\"project/data/raw/occupancy/DataOccupansi.csv\", \n",
    "                                     building_type=\"WITEL\"):\n",
    "    \"\"\"\n",
    "    Memproses data okupansi dengan building classification\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"🏢 MEMPROSES DATA OKUPANSI - {building_type}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Baca file CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"✅ Berhasil membaca file: {len(df)} baris data\")\n",
    "        \n",
    "        # Tampilkan info awal\n",
    "        print(f\"\\n📊 INFO DATA AWAL:\")\n",
    "        print(f\"Building Type: {building_type}\")\n",
    "        print(f\"Kolom tersedia: {list(df.columns)}\")\n",
    "        print(f\"Shape data: {df.shape}\")\n",
    "        \n",
    "        # Preview data\n",
    "        print(f\"\\n👀 PREVIEW DATA (5 baris pertama):\")\n",
    "        print(df.head().to_string())\n",
    "        \n",
    "        return df, building_type\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error membaca file: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3. FUNGSI IDENTIFIKASI DAN KONVERSI DATETIME\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def identifikasi_kolom_datetime(df):\n",
    "    \"\"\"\n",
    "    Mengidentifikasi kolom yang berisi informasi waktu/tanggal\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n🔍 MENGIDENTIFIKASI KOLOM DATETIME:\")\n",
    "    \n",
    "    datetime_candidates = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # Cek berdasarkan nama kolom\n",
    "        datetime_keywords = ['date', 'time', 'datetime', 'timestamp', 'tanggal', 'waktu', 'jam']\n",
    "        \n",
    "        if any(keyword in col.lower() for keyword in datetime_keywords):\n",
    "            datetime_candidates.append(col)\n",
    "            print(f\"  📅 Kandidat datetime: {col}\")\n",
    "        \n",
    "        # Cek berdasarkan sample data\n",
    "        sample_data = df[col].dropna().head(10)\n",
    "        for val in sample_data:\n",
    "            val_str = str(val)\n",
    "            # Cek format tanggal umum\n",
    "            if any(char in val_str for char in ['-', '/', ':']):\n",
    "                if len(val_str) > 8:  # Minimal length untuk datetime\n",
    "                    if col not in datetime_candidates:\n",
    "                        datetime_candidates.append(col)\n",
    "                        print(f\"  📅 Kandidat datetime berdasarkan format: {col}\")\n",
    "                    break\n",
    "    \n",
    "    return datetime_candidates\n",
    "\n",
    "def convert_to_datetime(df, datetime_col):\n",
    "    \"\"\"\n",
    "    Mengkonversi kolom ke datetime dengan berbagai format\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n⏰ MENGKONVERSI KOLOM {datetime_col} KE DATETIME:\")\n",
    "    \n",
    "    # Format datetime yang umum\n",
    "    datetime_formats = [\n",
    "        '%Y-%m-%d %H:%M:%S',\n",
    "        '%Y-%m-%d %H:%M',\n",
    "        '%Y-%m-%d',\n",
    "        '%d/%m/%Y %H:%M:%S',\n",
    "        '%d/%m/%Y %H:%M',\n",
    "        '%d/%m/%Y',\n",
    "        '%d-%m-%Y %H:%M:%S',\n",
    "        '%d-%m-%Y %H:%M',\n",
    "        '%d-%m-%Y',\n",
    "        '%Y/%m/%d %H:%M:%S',\n",
    "        '%Y/%m/%d %H:%M',\n",
    "        '%Y/%m/%d'\n",
    "    ]\n",
    "    \n",
    "    original_col = df[datetime_col].copy()\n",
    "    \n",
    "    # Coba konversi otomatis pandas\n",
    "    try:\n",
    "        df[datetime_col] = pd.to_datetime(df[datetime_col])\n",
    "        print(f\"  ✅ Berhasil konversi otomatis\")\n",
    "        return df\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Coba format satu per satu\n",
    "    for fmt in datetime_formats:\n",
    "        try:\n",
    "            df[datetime_col] = pd.to_datetime(original_col, format=fmt)\n",
    "            print(f\"  ✅ Berhasil dengan format: {fmt}\")\n",
    "            return df\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Jika gagal, coba konversi dengan errors='coerce'\n",
    "    try:\n",
    "        df[datetime_col] = pd.to_datetime(original_col, errors='coerce')\n",
    "        null_count = df[datetime_col].isnull().sum()\n",
    "        if null_count > 0:\n",
    "            print(f\"  ⚠️  Konversi berhasil tapi ada {null_count} nilai null\")\n",
    "        else:\n",
    "            print(f\"  ✅ Berhasil konversi dengan method coerce\")\n",
    "        return df\n",
    "    except:\n",
    "        print(f\"  ❌ Gagal mengkonversi kolom {datetime_col}\")\n",
    "        return df\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 4. ALIGN DENGAN ENERGY DATA DATES\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def get_energy_data_date_range(energy_file='all_energy_data.csv'):\n",
    "    \"\"\"\n",
    "    Mendapatkan rentang tanggal dari energy data untuk alignment\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n📅 MENGECEK RENTANG TANGGAL ENERGY DATA:\")\n",
    "    \n",
    "    try:\n",
    "        # Coba baca energy data\n",
    "        df_energy = pd.read_csv(energy_file)\n",
    "        \n",
    "        # Cari kolom datetime di energy data\n",
    "        datetime_candidates = []\n",
    "        for col in df_energy.columns:\n",
    "            if any(keyword in col.lower() for keyword in ['date', 'time', 'datetime', 'timestamp']):\n",
    "                datetime_candidates.append(col)\n",
    "        \n",
    "        if datetime_candidates:\n",
    "            datetime_col = datetime_candidates[0]\n",
    "            df_energy[datetime_col] = pd.to_datetime(df_energy[datetime_col], errors='coerce')\n",
    "            \n",
    "            min_date = df_energy[datetime_col].min().date()\n",
    "            max_date = df_energy[datetime_col].max().date()\n",
    "            \n",
    "            print(f\"  ✅ Energy data date range: {min_date} to {max_date}\")\n",
    "            return min_date, max_date, True\n",
    "        else:\n",
    "            print(f\"  ⚠️  Tidak ditemukan kolom datetime di energy data\")\n",
    "            return None, None, False\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"  ⚠️  File {energy_file} tidak ditemukan, menggunakan occupancy date range\")\n",
    "        return None, None, False\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error reading energy data: {e}\")\n",
    "        return None, None, False\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 5. AGREGASI HARIAN YANG SELARAS DENGAN ENERGY DATA\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def agregasi_harian_occupancy_energy_aligned(df, datetime_col, building_type, occupancy_cols=None, \n",
    "                                           energy_date_range=None):\n",
    "    \"\"\"\n",
    "    Mengagregasi data okupansi harian yang selaras dengan struktur energy data (FIXED)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n📈 MELAKUKAN AGREGASI HARIAN (ENERGY DATA ALIGNED - FIXED):\")\n",
    "    \n",
    "    # Buat kolom tanggal\n",
    "    df['date'] = df[datetime_col].dt.date\n",
    "    \n",
    "    # Filter berdasarkan energy date range jika tersedia\n",
    "    if energy_date_range and energy_date_range[2]:  # jika energy data tersedia\n",
    "        min_date, max_date, _ = energy_date_range\n",
    "        df = df[(df['date'] >= min_date) & (df['date'] <= max_date)]\n",
    "        print(f\"  📅 Filtered to energy date range: {min_date} to {max_date}\")\n",
    "        print(f\"  📊 Records after filtering: {len(df)}\")\n",
    "    \n",
    "    # Identifikasi kolom okupansi jika tidak diberikan\n",
    "    if occupancy_cols is None:\n",
    "        occupancy_keywords = ['occupancy', 'okupansi', 'jumlah', 'count', 'people', 'person', 'orang']\n",
    "        occupancy_cols = []\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if col != datetime_col and col != 'date':\n",
    "                if any(keyword in col.lower() for keyword in occupancy_keywords):\n",
    "                    occupancy_cols.append(col)\n",
    "                elif pd.api.types.is_numeric_dtype(df[col]):\n",
    "                    occupancy_cols.append(col)\n",
    "    \n",
    "    print(f\"  📊 Kolom okupansi yang akan diagregasi: {occupancy_cols}\")\n",
    "    \n",
    "    # === PERBAIKAN: Gunakan pendekatan yang berbeda untuk agregasi ===\n",
    "    \n",
    "    # 1. Lakukan agregasi per fungsi statistik\n",
    "    agg_results = []\n",
    "    \n",
    "    for col in occupancy_cols:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            # Agregasi per statistik\n",
    "            col_mean = df.groupby('date')[col].mean().round(2).to_frame(f\"{col}_mean\")\n",
    "            col_max = df.groupby('date')[col].max().round(2).to_frame(f\"{col}_max\")\n",
    "            col_min = df.groupby('date')[col].min().round(2).to_frame(f\"{col}_min\")\n",
    "            col_std = df.groupby('date')[col].std().round(2).to_frame(f\"{col}_std\")\n",
    "            col_count = df.groupby('date')[col].count().to_frame(f\"{col}_count\")\n",
    "            \n",
    "            # Gabungkan hasil agregasi untuk kolom ini\n",
    "            col_agg = pd.concat([col_mean, col_max, col_min, col_std, col_count], axis=1)\n",
    "            agg_results.append(col_agg)\n",
    "    \n",
    "    # 2. Gabungkan semua hasil agregasi\n",
    "    if agg_results:\n",
    "        daily_agg = pd.concat(agg_results, axis=1)\n",
    "        daily_agg = daily_agg.reset_index()\n",
    "    else:\n",
    "        # Fallback jika tidak ada kolom numerik\n",
    "        daily_agg = df.groupby('date').size().reset_index(name='total_records')\n",
    "    \n",
    "    # Tambahkan metadata yang konsisten dengan energy data\n",
    "    daily_agg = add_energy_metadata_structure(daily_agg, building_type, occupancy_cols)\n",
    "    \n",
    "    print(f\"  ✅ Berhasil agregasi: {len(daily_agg)} hari data\")\n",
    "    print(f\"  📊 Kolom hasil agregasi: {len(daily_agg.columns)} kolom\")\n",
    "    \n",
    "    return daily_agg\n",
    "\n",
    "def add_energy_metadata_structure(df_daily, building_type, occupancy_cols):\n",
    "    \"\"\"\n",
    "    Menambahkan struktur metadata yang konsisten dengan energy data\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"  🏗️  Menambahkan metadata struktur energy data...\")\n",
    "    \n",
    "    # Tentukan system type berdasarkan kolom occupancy\n",
    "    if len(occupancy_cols) > 0:\n",
    "        main_col = occupancy_cols[0].lower()\n",
    "        if 'count' in main_col or 'jumlah' in main_col:\n",
    "            system_type = 'OCCUPANCY_COUNTER'\n",
    "        elif 'people' in main_col or 'person' in main_col or 'orang' in main_col:\n",
    "            system_type = 'PEOPLE_COUNT'\n",
    "        else:\n",
    "            system_type = 'OCCUPANCY_SENSOR'\n",
    "    else:\n",
    "        system_type = 'OCCUPANCY_SENSOR'\n",
    "    \n",
    "    # Dapatkan meter ID\n",
    "    meter_id = get_occupancy_meter_id(building_type, system_type)\n",
    "    \n",
    "    # Insert kolom metadata di depan (mengikuti urutan energy data)\n",
    "    df_daily.insert(0, 'record_id', range(1, len(df_daily) + 1))\n",
    "    \n",
    "    # Tambahkan kolom metadata lainnya\n",
    "    df_daily['building'] = building_type\n",
    "    df_daily['category'] = 'OCCUPANCY'\n",
    "    df_daily['system_type'] = system_type\n",
    "    df_daily['level'] = 'General'  # occupancy tidak memiliki level L1-L8\n",
    "    df_daily['meter_id'] = meter_id\n",
    "    df_daily['description'] = f'{building_type} {system_type} Daily Aggregated Data'\n",
    "    df_daily['source_file'] = 'DataOccupansi.csv'\n",
    "    df_daily['file_path'] = 'project/data/raw/occupancy/'\n",
    "    df_daily['processed_ts'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    return df_daily\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 6. PROSES UNTUK KEDUA BUILDING (WITEL & OPMC)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def process_occupancy_for_both_buildings(file_path=\"project/data/raw/occupancy/DataOccupansi.csv\"):\n",
    "    \"\"\"\n",
    "    Memproses occupancy data untuk kedua building (WITEL & OPMC)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🚀 MEMPROSES OCCUPANCY UNTUK WITEL & OPMC\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Get energy data date range untuk alignment\n",
    "    energy_date_range = get_energy_data_date_range()\n",
    "    \n",
    "    all_occupancy_data = []\n",
    "    \n",
    "    # Process untuk kedua building\n",
    "    for building in ['WITEL', 'OPMC']:\n",
    "        \n",
    "        print(f\"\\n🏢 PROCESSING {building}:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # 1. Baca data dengan building classification\n",
    "        df, building_type = proses_agregasi_occupancy_enhanced(file_path, building)\n",
    "        \n",
    "        if df is None:\n",
    "            print(f\"❌ Gagal memproses data untuk {building}\")\n",
    "            continue\n",
    "        \n",
    "        # 2. Identifikasi dan convert datetime\n",
    "        datetime_candidates = identifikasi_kolom_datetime(df)\n",
    "        \n",
    "        if not datetime_candidates:\n",
    "            print(f\"❌ Tidak ditemukan kolom datetime untuk {building}!\")\n",
    "            continue\n",
    "        \n",
    "        datetime_col = datetime_candidates[0]\n",
    "        print(f\"✅ Menggunakan kolom datetime: {datetime_col}\")\n",
    "        \n",
    "        df = convert_to_datetime(df, datetime_col)\n",
    "        \n",
    "        # 3. Agregasi harian dengan energy alignment\n",
    "        df_daily = agregasi_harian_occupancy_energy_aligned(\n",
    "            df, datetime_col, building_type, energy_date_range=energy_date_range\n",
    "        )\n",
    "        \n",
    "        # 4. Simpan individual file\n",
    "        output_file = f'{building.lower()}_occupancy_daily.csv'\n",
    "        df_daily.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        print(f\"💾 Data {building} disimpan ke: {output_file}\")\n",
    "        \n",
    "        all_occupancy_data.append(df_daily)\n",
    "    \n",
    "    return all_occupancy_data\n",
    "\n",
    "def create_combined_occupancy_data(all_occupancy_data):\n",
    "    \"\"\"\n",
    "    Menggabungkan occupancy data dari semua building\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n🔗 MENGGABUNGKAN OCCUPANCY DATA:\")\n",
    "    \n",
    "    if not all_occupancy_data:\n",
    "        print(\"❌ Tidak ada data untuk digabungkan!\")\n",
    "        return None\n",
    "    \n",
    "    # Gabungkan semua data\n",
    "    df_combined = pd.concat(all_occupancy_data, ignore_index=True)\n",
    "    \n",
    "    # Update record_id\n",
    "    df_combined['record_id'] = range(1, len(df_combined) + 1)\n",
    "    \n",
    "    # Simpan combined file\n",
    "    output_file = 'all_occupancy_data.csv'\n",
    "    df_combined.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"✅ Data gabungan: {len(df_combined)} records\")\n",
    "    print(f\"💾 Disimpan ke: {output_file}\")\n",
    "    \n",
    "    # Preview hasil (mengikuti format energy data)\n",
    "    preview_cols = ['record_id', 'date', 'building', 'meter_id', 'system_type', \n",
    "                   'level', 'description', 'category']\n",
    "    \n",
    "    # Tambahkan beberapa kolom occupancy untuk preview\n",
    "    occupancy_cols = [col for col in df_combined.columns if '_mean' in col][:2]\n",
    "    preview_cols.extend(occupancy_cols)\n",
    "    \n",
    "    available_cols = [col for col in preview_cols if col in df_combined.columns]\n",
    "    \n",
    "    print(f\"\\n👀 PREVIEW HASIL GABUNGAN:\")\n",
    "    print(\"=\" * 100)\n",
    "    print(df_combined[available_cols].head(10).to_string(index=False))\n",
    "    \n",
    "    return df_combined\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 7. INTEGRASI DENGAN ENERGY DATA\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def integrate_occupancy_with_energy_data(occupancy_file='all_occupancy_data.csv', \n",
    "                                       energy_file='all_energy_data.csv'):\n",
    "    \"\"\"\n",
    "    Mengintegrasikan occupancy data dengan energy data\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n🔄 MENGINTEGRASIKAN DENGAN ENERGY DATA:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Baca kedua file\n",
    "        df_occupancy = pd.read_csv(occupancy_file)\n",
    "        df_energy = pd.read_csv(energy_file)\n",
    "        \n",
    "        print(f\"📊 Occupancy records: {len(df_occupancy)}\")\n",
    "        print(f\"⚡ Energy records: {len(df_energy)}\")\n",
    "        \n",
    "        # Gabungkan data\n",
    "        df_integrated = pd.concat([df_energy, df_occupancy], ignore_index=True)\n",
    "        \n",
    "        # Update record_id untuk semua data\n",
    "        df_integrated['record_id'] = range(1, len(df_integrated) + 1)\n",
    "        \n",
    "        # Simpan hasil integrasi\n",
    "        output_file = 'all_building_integrated_data.csv'\n",
    "        df_integrated.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        \n",
    "        print(f\"✅ Data terintegrasi: {len(df_integrated)} records\")\n",
    "        print(f\"💾 Disimpan ke: {output_file}\")\n",
    "        \n",
    "        # Ringkasan integrasi\n",
    "        summary = df_integrated.groupby(['building', 'category']).size().reset_index(name='count')\n",
    "        print(f\"\\n📈 RINGKASAN INTEGRASI:\")\n",
    "        print(summary.to_string(index=False))\n",
    "        \n",
    "        return df_integrated\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error integrasi: {e}\")\n",
    "        return None\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 8. FUNGSI UTAMA\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def main_process_occupancy_energy_aligned():\n",
    "    \"\"\"\n",
    "    Fungsi utama untuk memproses occupancy data yang selaras dengan energy data\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🌟 STARTING OCCUPANCY PROCESSING - ENERGY DATA ALIGNED\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"📋 Requirements:\")\n",
    "    print(\"   1. ✅ Building classification: WITEL & OPMC\")\n",
    "    print(\"   2. ✅ Meter ID mengikuti pola energy meter\")\n",
    "    print(\"   3. ✅ Level: General (tidak ada L1-L8)\")\n",
    "    print(\"   4. ✅ Date alignment dengan energy data\")\n",
    "    print(\"   5. ✅ Agregasi harian seperti energy data\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. Process occupancy untuk kedua building\n",
    "    all_occupancy_data = process_occupancy_for_both_buildings()\n",
    "    \n",
    "    if not all_occupancy_data:\n",
    "        print(\"❌ Tidak berhasil memproses occupancy data!\")\n",
    "        return None\n",
    "    \n",
    "    # 2. Gabungkan occupancy data\n",
    "    df_occupancy_combined = create_combined_occupancy_data(all_occupancy_data)\n",
    "    \n",
    "    if df_occupancy_combined is None:\n",
    "        print(\"❌ Gagal menggabungkan occupancy data!\")\n",
    "        return None\n",
    "    \n",
    "    # 3. Integrasi dengan energy data\n",
    "    df_integrated = integrate_occupancy_with_energy_data()\n",
    "    \n",
    "    # 4. Final summary\n",
    "    print(f\"\\n🎉 PROSES SELESAI!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"📁 Files yang dihasilkan:\")\n",
    "    print(\"   • witel_occupancy_daily.csv\")\n",
    "    print(\"   • opmc_occupancy_daily.csv\") \n",
    "    print(\"   • all_occupancy_data.csv\")\n",
    "    if df_integrated is not None:\n",
    "        print(\"   • all_building_integrated_data.csv\")\n",
    "    \n",
    "    print(f\"\\n✨ Occupancy data sudah selaras dengan energy data structure!\")\n",
    "    \n",
    "    return df_integrated\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 9. EKSEKUSI\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hasil_integrasi = main_process_occupancy_energy_aligned()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "751f72a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌟 STARTING TEMPERATURE PROCESSING - ENERGY DATA ALIGNED\n",
      "================================================================================\n",
      "📋 Requirements:\n",
      "   1. ✅ Building classification: WITEL & OPMC\n",
      "   2. ✅ Meter ID mengikuti pola energy meter\n",
      "   3. ✅ Level: L1-L8 berdasarkan lantai, atau General\n",
      "   4. ✅ Date alignment dengan energy data\n",
      "   5. ✅ Agregasi harian seperti energy data\n",
      "================================================================================\n",
      "🚀 MEMPROSES TEMPERATURE UNTUK WITEL & OPMC\n",
      "======================================================================\n",
      "\n",
      "📅 MENGECEK RENTANG TANGGAL ENERGY DATA:\n",
      "  ✅ Energy data date range: 2024-01-01 to 2024-12-31\n",
      "\n",
      "🏢 PROCESSING WITEL:\n",
      "----------------------------------------\n",
      "🌡️ MEMPROSES DATA SUHU - WITEL\n",
      "==================================================\n",
      "✅ Berhasil membaca file: 53436 baris data\n",
      "\n",
      "📊 INFO DATA AWAL:\n",
      "Building Type: WITEL\n",
      "Kolom tersedia: ['Timestamp', 'Nama Gedung', 'Lantai', 'Ruangan', 'Hasil Ukur Suhu (Celsius)', 'Hasil Ukur Lumens (lx)']\n",
      "Shape data: (53436, 6)\n",
      "\n",
      "👀 PREVIEW DATA (5 baris pertama):\n",
      "    Timestamp Nama Gedung  Lantai             Ruangan  Hasil Ukur Suhu (Celsius)  Hasil Ukur Lumens (lx)\n",
      "0  2024-01-01       WITEL       1        Ruang Satpam                         31                     644\n",
      "1  2024-01-01       WITEL       1      Ruang Workshop                         30                      97\n",
      "2  2024-01-01       WITEL       1        Ruang Gudang                         26                     283\n",
      "3  2024-01-01       WITEL       1      Ruang Pool KBM                         30                     151\n",
      "4  2024-01-01       WITEL       1  Ruang Parkir Motor                         34                     146\n",
      "\n",
      "🔍 AUTO-DETECTING KOLOM:\n",
      "  📅 DateTime column: Timestamp\n",
      "  🏢 Building column: Nama Gedung\n",
      "  🏗️ Floor column: Lantai\n",
      "  🌡️ Temperature column: Hasil Ukur Suhu (Celsius)\n",
      "\n",
      "📈 MELAKUKAN AGREGASI HARIAN (ENERGY DATA ALIGNED):\n",
      "  📅 Filtered to energy date range: 2024-01-01 to 2024-12-31\n",
      "  📊 Records after filtering: 53436\n",
      "  📊 Grouping berdasarkan: ['date', 'Nama Gedung', 'Lantai']\n",
      "  🌡️ Kolom suhu: ['Hasil Ukur Suhu (Celsius)']\n",
      "  🏗️  Menambahkan metadata struktur energy data...\n",
      "  ✅ Berhasil agregasi: 4758 kombinasi hari-lokasi\n",
      "💾 Data WITEL disimpan ke: witel_temperature_daily.csv\n",
      "\n",
      "🏢 PROCESSING OPMC:\n",
      "----------------------------------------\n",
      "🌡️ MEMPROSES DATA SUHU - OPMC\n",
      "==================================================\n",
      "✅ Berhasil membaca file: 53436 baris data\n",
      "\n",
      "📊 INFO DATA AWAL:\n",
      "Building Type: OPMC\n",
      "Kolom tersedia: ['Timestamp', 'Nama Gedung', 'Lantai', 'Ruangan', 'Hasil Ukur Suhu (Celsius)', 'Hasil Ukur Lumens (lx)']\n",
      "Shape data: (53436, 6)\n",
      "\n",
      "👀 PREVIEW DATA (5 baris pertama):\n",
      "    Timestamp Nama Gedung  Lantai             Ruangan  Hasil Ukur Suhu (Celsius)  Hasil Ukur Lumens (lx)\n",
      "0  2024-01-01       WITEL       1        Ruang Satpam                         31                     644\n",
      "1  2024-01-01       WITEL       1      Ruang Workshop                         30                      97\n",
      "2  2024-01-01       WITEL       1        Ruang Gudang                         26                     283\n",
      "3  2024-01-01       WITEL       1      Ruang Pool KBM                         30                     151\n",
      "4  2024-01-01       WITEL       1  Ruang Parkir Motor                         34                     146\n",
      "\n",
      "🔍 AUTO-DETECTING KOLOM:\n",
      "  📅 DateTime column: Timestamp\n",
      "  🏢 Building column: Nama Gedung\n",
      "  🏗️ Floor column: Lantai\n",
      "  🌡️ Temperature column: Hasil Ukur Suhu (Celsius)\n",
      "\n",
      "📈 MELAKUKAN AGREGASI HARIAN (ENERGY DATA ALIGNED):\n",
      "  📅 Filtered to energy date range: 2024-01-01 to 2024-12-31\n",
      "  📊 Records after filtering: 53436\n",
      "  📊 Grouping berdasarkan: ['date', 'Nama Gedung', 'Lantai']\n",
      "  🌡️ Kolom suhu: ['Hasil Ukur Suhu (Celsius)']\n",
      "  🏗️  Menambahkan metadata struktur energy data...\n",
      "  ✅ Berhasil agregasi: 4758 kombinasi hari-lokasi\n",
      "💾 Data OPMC disimpan ke: opmc_temperature_daily.csv\n",
      "\n",
      "🔗 MENGGABUNGKAN TEMPERATURE DATA:\n",
      "✅ Data gabungan: 9516 records\n",
      "💾 Disimpan ke: all_temperature_data.csv\n",
      "\n",
      "👀 PREVIEW HASIL GABUNGAN:\n",
      "========================================================================================================================\n",
      " record_id       date building  meter_id system_type level                                 description    category\n",
      "         1 2024-01-01    WITEL 251401002 TEMP_SENSOR    L2 WITEL TEMP_SENSOR L2 Daily Temperature Data TEMPERATURE\n",
      "         2 2024-01-01    WITEL 251401003 TEMP_SENSOR    L3 WITEL TEMP_SENSOR L3 Daily Temperature Data TEMPERATURE\n",
      "         3 2024-01-01    WITEL 251401005 TEMP_SENSOR    L5 WITEL TEMP_SENSOR L5 Daily Temperature Data TEMPERATURE\n",
      "         4 2024-01-01    WITEL 251401006 TEMP_SENSOR    L6 WITEL TEMP_SENSOR L6 Daily Temperature Data TEMPERATURE\n",
      "         5 2024-01-01    WITEL 251401001 TEMP_SENSOR    L1 WITEL TEMP_SENSOR L1 Daily Temperature Data TEMPERATURE\n",
      "         6 2024-01-01    WITEL 251401001 TEMP_SENSOR    L1 WITEL TEMP_SENSOR L1 Daily Temperature Data TEMPERATURE\n",
      "         7 2024-01-01    WITEL 251401002 TEMP_SENSOR    L2 WITEL TEMP_SENSOR L2 Daily Temperature Data TEMPERATURE\n",
      "         8 2024-01-01    WITEL 251401003 TEMP_SENSOR    L3 WITEL TEMP_SENSOR L3 Daily Temperature Data TEMPERATURE\n",
      "         9 2024-01-01    WITEL 251401004 TEMP_SENSOR    L4 WITEL TEMP_SENSOR L4 Daily Temperature Data TEMPERATURE\n",
      "        10 2024-01-01    WITEL 251401005 TEMP_SENSOR    L5 WITEL TEMP_SENSOR L5 Daily Temperature Data TEMPERATURE\n",
      "\n",
      "🔄 MENGINTEGRASIKAN DENGAN BUILDING DATA:\n",
      "============================================================\n",
      "⚠️  File all_building_integrated_data.csv tidak ditemukan, hanya temperature data\n",
      "✅ Data terintegrasi lengkap: 9516 records\n",
      "💾 Disimpan ke: all_building_complete_data.csv\n",
      "\n",
      "📈 RINGKASAN INTEGRASI LENGKAP:\n",
      "building    category  count\n",
      "    OPMC TEMPERATURE   4758\n",
      "   WITEL TEMPERATURE   4758\n",
      "\n",
      "🎉 PROSES SELESAI!\n",
      "==================================================\n",
      "📁 Files yang dihasilkan:\n",
      "   • witel_temperature_daily.csv\n",
      "   • opmc_temperature_daily.csv\n",
      "   • all_temperature_data.csv\n",
      "   • all_building_complete_data.csv\n",
      "\n",
      "✨ Temperature data sudah selaras dengan energy data structure!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1. METER ID MAPPING UNTUK TEMPERATURE (MENGIKUTI POLA ENERGY)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Synthetic Meter IDs untuk Temperature (mengikuti pola energy meter)\n",
    "METER_MAP_TEMPERATURE = {\n",
    "    'WITEL': {\n",
    "        ('TEMP_SENSOR', 'L1'): '251401001', ('TEMP_SENSOR', 'L2'): '251401002',\n",
    "        ('TEMP_SENSOR', 'L3'): '251401003', ('TEMP_SENSOR', 'L4'): '251401004',\n",
    "        ('TEMP_SENSOR', 'L5'): '251401005', ('TEMP_SENSOR', 'L6'): '251401006',\n",
    "        ('TEMP_SENSOR', 'L7'): '251401007', ('TEMP_SENSOR', 'L8'): '251401008',\n",
    "        ('TEMP_SENSOR', 'General'): '251401009',\n",
    "        ('HUMIDITY_SENSOR', 'General'): '251401010',\n",
    "        ('AMBIENT_MONITOR', 'General'): '251401011',\n",
    "    },\n",
    "    'OPMC': {\n",
    "        ('TEMP_SENSOR', 'L1'): '251401101', ('TEMP_SENSOR', 'L2'): '251401102',\n",
    "        ('TEMP_SENSOR', 'L3'): '251401103', ('TEMP_SENSOR', 'L4'): '251401104',\n",
    "        ('TEMP_SENSOR', 'L5'): '251401105', ('TEMP_SENSOR', 'L6'): '251401106',\n",
    "        ('TEMP_SENSOR', 'L7'): '251401107', ('TEMP_SENSOR', 'L8'): '251401108',\n",
    "        ('TEMP_SENSOR', 'General'): '251401109',\n",
    "        ('HUMIDITY_SENSOR', 'General'): '251401110',\n",
    "        ('AMBIENT_MONITOR', 'General'): '251401111',\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_temperature_meter_id(building, system_type='TEMP_SENSOR', level='General'):\n",
    "    \"\"\"\n",
    "    Mendapatkan meter ID untuk temperature berdasarkan building, system type, dan level\n",
    "    \"\"\"\n",
    "    key = (system_type, level)\n",
    "    return METER_MAP_TEMPERATURE.get(building, {}).get(key, f'TEMP_{building}_{level}_001')\n",
    "\n",
    "def detect_level_from_floor(floor_info):\n",
    "    \"\"\"\n",
    "    Mendeteksi level dari informasi lantai\n",
    "    \"\"\"\n",
    "    if pd.isna(floor_info):\n",
    "        return 'General'\n",
    "    \n",
    "    floor_str = str(floor_info).upper()\n",
    "    \n",
    "    # Ekstrak angka lantai\n",
    "    import re\n",
    "    level_match = re.search(r'(\\d+)', floor_str)\n",
    "    \n",
    "    if level_match:\n",
    "        level_num = int(level_match.group(1))\n",
    "        if 1 <= level_num <= 8:\n",
    "            return f'L{level_num}'\n",
    "    \n",
    "    return 'General'\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2. FUNGSI BACA DATA DENGAN BUILDING CLASSIFICATION\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def proses_agregasi_suhu_enhanced(\n",
    "    file_path='project/data/raw/temperature/survey_suhu_2024_daily2.csv',\n",
    "    building_type=\"WITEL\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Memproses data suhu dengan building classification dan struktur energy data\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"🌡️ MEMPROSES DATA SUHU - {building_type}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Baca file CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"✅ Berhasil membaca file: {len(df)} baris data\")\n",
    "        \n",
    "        # Tampilkan info awal\n",
    "        print(f\"\\n📊 INFO DATA AWAL:\")\n",
    "        print(f\"Building Type: {building_type}\")\n",
    "        print(f\"Kolom tersedia: {list(df.columns)}\")\n",
    "        print(f\"Shape data: {df.shape}\")\n",
    "        \n",
    "        # Preview data\n",
    "        print(f\"\\n👀 PREVIEW DATA (5 baris pertama):\")\n",
    "        print(df.head().to_string())\n",
    "        \n",
    "        return df, building_type\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error membaca file: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3. AUTO-DETECT KOLOM DENGAN ENHANCED LOGIC\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def auto_detect_columns_enhanced(df):\n",
    "    \"\"\"\n",
    "    Auto-detect kolom dengan logika yang lebih canggih\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n🔍 AUTO-DETECTING KOLOM:\")\n",
    "    \n",
    "    detected = {\n",
    "        'datetime_col': None,\n",
    "        'suhu_cols': [],\n",
    "        'gedung_col': None,\n",
    "        'lantai_col': None,\n",
    "        'humidity_cols': []\n",
    "    }\n",
    "    \n",
    "    # Auto-detect datetime\n",
    "    datetime_keywords = ['date', 'time', 'datetime', 'timestamp', 'tanggal', 'waktu', 'jam']\n",
    "    for col in df.columns:\n",
    "        if any(keyword in col.lower() for keyword in datetime_keywords):\n",
    "            detected['datetime_col'] = col\n",
    "            print(f\"  📅 DateTime column: {col}\")\n",
    "            break\n",
    "    \n",
    "    # Auto-detect gedung\n",
    "    gedung_keywords = ['building', 'gedung', 'bangunan', 'tower', 'lokasi', 'location']\n",
    "    for col in df.columns:\n",
    "        if any(keyword in col.lower() for keyword in gedung_keywords):\n",
    "            detected['gedung_col'] = col\n",
    "            print(f\"  🏢 Building column: {col}\")\n",
    "            break\n",
    "    \n",
    "    # Auto-detect lantai\n",
    "    lantai_keywords = ['floor', 'lantai', 'level', 'tingkat']\n",
    "    for col in df.columns:\n",
    "        if any(keyword in col.lower() for keyword in lantai_keywords):\n",
    "            detected['lantai_col'] = col\n",
    "            print(f\"  🏗️ Floor column: {col}\")\n",
    "            break\n",
    "    \n",
    "    # Auto-detect suhu\n",
    "    suhu_keywords = ['temp', 'temperature', 'suhu', 'celcius', 'celsius', '°c', 'deg']\n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        if any(keyword in col_lower for keyword in suhu_keywords):\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                detected['suhu_cols'].append(col)\n",
    "                print(f\"  🌡️ Temperature column: {col}\")\n",
    "    \n",
    "    # Auto-detect humidity\n",
    "    humidity_keywords = ['humidity', 'kelembaban', 'rh', 'relative']\n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        if any(keyword in col_lower for keyword in humidity_keywords):\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                detected['humidity_cols'].append(col)\n",
    "                print(f\"  💧 Humidity column: {col}\")\n",
    "    \n",
    "    # Fallback untuk kolom numerik jika suhu tidak ditemukan\n",
    "    if not detected['suhu_cols']:\n",
    "        excluded_cols = [detected['datetime_col'], detected['gedung_col'], detected['lantai_col']]\n",
    "        for col in df.columns:\n",
    "            if col not in excluded_cols and pd.api.types.is_numeric_dtype(df[col]):\n",
    "                detected['suhu_cols'].append(col)\n",
    "                print(f\"  🌡️ Numeric column (assumed temp): {col}\")\n",
    "    \n",
    "    return detected\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 4. ALIGN DENGAN ENERGY DATA DATES\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def get_energy_data_date_range(energy_file='all_energy_data.csv'):\n",
    "    \"\"\"\n",
    "    Mendapatkan rentang tanggal dari energy data untuk alignment\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n📅 MENGECEK RENTANG TANGGAL ENERGY DATA:\")\n",
    "    \n",
    "    try:\n",
    "        df_energy = pd.read_csv(energy_file)\n",
    "        \n",
    "        datetime_candidates = []\n",
    "        for col in df_energy.columns:\n",
    "            if any(keyword in col.lower() for keyword in ['date', 'time', 'datetime', 'timestamp']):\n",
    "                datetime_candidates.append(col)\n",
    "        \n",
    "        if datetime_candidates:\n",
    "            datetime_col = datetime_candidates[0]\n",
    "            df_energy[datetime_col] = pd.to_datetime(df_energy[datetime_col], errors='coerce')\n",
    "            \n",
    "            min_date = df_energy[datetime_col].min().date()\n",
    "            max_date = df_energy[datetime_col].max().date()\n",
    "            \n",
    "            print(f\"  ✅ Energy data date range: {min_date} to {max_date}\")\n",
    "            return min_date, max_date, True\n",
    "        else:\n",
    "            print(f\"  ⚠️  Tidak ditemukan kolom datetime di energy data\")\n",
    "            return None, None, False\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"  ⚠️  File {energy_file} tidak ditemukan\")\n",
    "        return None, None, False\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error reading energy data: {e}\")\n",
    "        return None, None, False\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 5. AGREGASI HARIAN YANG SELARAS DENGAN ENERGY DATA\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def agregasi_harian_suhu_energy_aligned(df, detected_cols, building_type, energy_date_range=None):\n",
    "    \"\"\"\n",
    "    Mengagregasi data suhu harian yang selaras dengan struktur energy data\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n📈 MELAKUKAN AGREGASI HARIAN (ENERGY DATA ALIGNED):\")\n",
    "    \n",
    "    datetime_col = detected_cols['datetime_col']\n",
    "    suhu_cols = detected_cols['suhu_cols']\n",
    "    gedung_col = detected_cols['gedung_col']\n",
    "    lantai_col = detected_cols['lantai_col']\n",
    "    humidity_cols = detected_cols['humidity_cols']\n",
    "    \n",
    "    if not datetime_col:\n",
    "        raise ValueError(\"❌ Kolom datetime tidak ditemukan!\")\n",
    "    \n",
    "    if not suhu_cols:\n",
    "        raise ValueError(\"❌ Kolom suhu tidak ditemukan!\")\n",
    "    \n",
    "    # Konversi datetime\n",
    "    df[datetime_col] = pd.to_datetime(df[datetime_col], errors='coerce')\n",
    "    df = df.dropna(subset=[datetime_col])\n",
    "    \n",
    "    # Buat kolom tanggal\n",
    "    df['date'] = df[datetime_col].dt.date\n",
    "    \n",
    "    # Filter berdasarkan energy date range jika tersedia\n",
    "    if energy_date_range and energy_date_range[2]:\n",
    "        min_date, max_date, _ = energy_date_range\n",
    "        df = df[(df['date'] >= min_date) & (df['date'] <= max_date)]\n",
    "        print(f\"  📅 Filtered to energy date range: {min_date} to {max_date}\")\n",
    "        print(f\"  📊 Records after filtering: {len(df)}\")\n",
    "    \n",
    "    # Tentukan grouping columns\n",
    "    groupby_cols = ['date']\n",
    "    \n",
    "    # Tambahkan building jika tidak ada gedung_col\n",
    "    if not gedung_col:\n",
    "        df['building_temp'] = building_type\n",
    "        groupby_cols.append('building_temp')\n",
    "    else:\n",
    "        groupby_cols.append(gedung_col)\n",
    "    \n",
    "    # Tambahkan lantai jika ada\n",
    "    if lantai_col:\n",
    "        groupby_cols.append(lantai_col)\n",
    "    \n",
    "    print(f\"  📊 Grouping berdasarkan: {groupby_cols}\")\n",
    "    print(f\"  🌡️ Kolom suhu: {suhu_cols}\")\n",
    "    \n",
    "    # Lakukan agregasi per kolom (hindari nested dictionary)\n",
    "    agg_results = []\n",
    "    \n",
    "    # Agregasi suhu\n",
    "    for col in suhu_cols:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            col_mean = df.groupby(groupby_cols)[col].mean().round(2).to_frame(f\"{col}_mean\")\n",
    "            col_max = df.groupby(groupby_cols)[col].max().round(2).to_frame(f\"{col}_max\")\n",
    "            col_min = df.groupby(groupby_cols)[col].min().round(2).to_frame(f\"{col}_min\")\n",
    "            col_std = df.groupby(groupby_cols)[col].std().round(2).to_frame(f\"{col}_std\")\n",
    "            col_count = df.groupby(groupby_cols)[col].count().to_frame(f\"{col}_count\")\n",
    "            \n",
    "            col_agg = pd.concat([col_mean, col_max, col_min, col_std, col_count], axis=1)\n",
    "            agg_results.append(col_agg)\n",
    "    \n",
    "    # Agregasi humidity jika ada\n",
    "    for col in humidity_cols:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            col_mean = df.groupby(groupby_cols)[col].mean().round(2).to_frame(f\"{col}_mean\")\n",
    "            col_max = df.groupby(groupby_cols)[col].max().round(2).to_frame(f\"{col}_max\")\n",
    "            col_min = df.groupby(groupby_cols)[col].min().round(2).to_frame(f\"{col}_min\")\n",
    "            col_std = df.groupby(groupby_cols)[col].std().round(2).to_frame(f\"{col}_std\")\n",
    "            col_count = df.groupby(groupby_cols)[col].count().to_frame(f\"{col}_count\")\n",
    "            \n",
    "            col_agg = pd.concat([col_mean, col_max, col_min, col_std, col_count], axis=1)\n",
    "            agg_results.append(col_agg)\n",
    "    \n",
    "    # Gabungkan semua hasil agregasi\n",
    "    if agg_results:\n",
    "        daily_agg = pd.concat(agg_results, axis=1)\n",
    "        daily_agg = daily_agg.reset_index()\n",
    "    else:\n",
    "        daily_agg = df.groupby(groupby_cols).size().reset_index(name='total_records')\n",
    "    \n",
    "    # Tambahkan metadata energy data structure\n",
    "    daily_agg = add_temperature_metadata_structure(daily_agg, building_type, detected_cols)\n",
    "    \n",
    "    print(f\"  ✅ Berhasil agregasi: {len(daily_agg)} kombinasi hari-lokasi\")\n",
    "    \n",
    "    return daily_agg\n",
    "\n",
    "def add_temperature_metadata_structure(df_daily, building_type, detected_cols):\n",
    "    \"\"\"\n",
    "    Menambahkan struktur metadata yang konsisten dengan energy data\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"  🏗️  Menambahkan metadata struktur energy data...\")\n",
    "    \n",
    "    lantai_col = detected_cols['lantai_col']\n",
    "    gedung_col = detected_cols['gedung_col']\n",
    "    \n",
    "    # Tentukan system type dan level untuk setiap row\n",
    "    df_daily['level'] = 'General'  # default\n",
    "    df_daily['system_type'] = 'TEMP_SENSOR'  # default\n",
    "    \n",
    "    # Jika ada kolom lantai, extract level\n",
    "    if lantai_col and lantai_col in df_daily.columns:\n",
    "        df_daily['level'] = df_daily[lantai_col].apply(detect_level_from_floor)\n",
    "    \n",
    "    # Jika ada kolom humidity, update system type\n",
    "    if detected_cols['humidity_cols']:\n",
    "        df_daily['system_type'] = 'AMBIENT_MONITOR'\n",
    "    \n",
    "    # Generate meter ID untuk setiap row\n",
    "    df_daily['meter_id'] = df_daily.apply(\n",
    "        lambda row: get_temperature_meter_id(building_type, row['system_type'], row['level']), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Insert metadata di depan\n",
    "    df_daily.insert(0, 'record_id', range(1, len(df_daily) + 1))\n",
    "    \n",
    "    # Tambahkan kolom metadata lainnya\n",
    "    df_daily['building'] = building_type\n",
    "    df_daily['category'] = 'TEMPERATURE'\n",
    "    \n",
    "    # Description berdasarkan lokasi\n",
    "    df_daily['description'] = df_daily.apply(\n",
    "        lambda row: f\"{building_type} {row['system_type']} {row['level']} Daily Temperature Data\", \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    df_daily['source_file'] = 'survey_suhu_2024_daily2.csv'\n",
    "    df_daily['file_path'] = 'project/data/raw/temperature/'\n",
    "    df_daily['processed_ts'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    return df_daily\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 6. PROSES UNTUK KEDUA BUILDING (WITEL & OPMC)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def process_temperature_for_both_buildings(\n",
    "    file_path='project/data/raw/temperature/survey_suhu_2024_daily2.csv'\n",
    "):\n",
    "    \"\"\"\n",
    "    Memproses temperature data untuk kedua building (WITEL & OPMC)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🚀 MEMPROSES TEMPERATURE UNTUK WITEL & OPMC\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Get energy data date range untuk alignment\n",
    "    energy_date_range = get_energy_data_date_range()\n",
    "    \n",
    "    all_temperature_data = []\n",
    "    \n",
    "    # Process untuk kedua building\n",
    "    for building in ['WITEL', 'OPMC']:\n",
    "        \n",
    "        print(f\"\\n🏢 PROCESSING {building}:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            # 1. Baca data dengan building classification\n",
    "            df, building_type = proses_agregasi_suhu_enhanced(file_path, building)\n",
    "            \n",
    "            if df is None:\n",
    "                print(f\"❌ Gagal memproses data untuk {building}\")\n",
    "                continue\n",
    "            \n",
    "            # 2. Auto-detect kolom\n",
    "            detected_cols = auto_detect_columns_enhanced(df)\n",
    "            \n",
    "            if not detected_cols['datetime_col']:\n",
    "                print(f\"❌ Tidak ditemukan kolom datetime untuk {building}!\")\n",
    "                continue\n",
    "            \n",
    "            # 3. Agregasi harian dengan energy alignment\n",
    "            df_daily = agregasi_harian_suhu_energy_aligned(\n",
    "                df, detected_cols, building_type, energy_date_range=energy_date_range\n",
    "            )\n",
    "            \n",
    "            # 4. Simpan individual file\n",
    "            output_file = f'{building.lower()}_temperature_daily.csv'\n",
    "            df_daily.to_csv(output_file, index=False, encoding='utf-8')\n",
    "            print(f\"💾 Data {building} disimpan ke: {output_file}\")\n",
    "            \n",
    "            all_temperature_data.append(df_daily)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {building}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return all_temperature_data\n",
    "\n",
    "def create_combined_temperature_data(all_temperature_data):\n",
    "    \"\"\"\n",
    "    Menggabungkan temperature data dari semua building\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n🔗 MENGGABUNGKAN TEMPERATURE DATA:\")\n",
    "    \n",
    "    if not all_temperature_data:\n",
    "        print(\"❌ Tidak ada data untuk digabungkan!\")\n",
    "        return None\n",
    "    \n",
    "    # Gabungkan semua data\n",
    "    df_combined = pd.concat(all_temperature_data, ignore_index=True)\n",
    "    \n",
    "    # Update record_id\n",
    "    df_combined['record_id'] = range(1, len(df_combined) + 1)\n",
    "    \n",
    "    # Simpan combined file\n",
    "    output_file = 'all_temperature_data.csv'\n",
    "    df_combined.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"✅ Data gabungan: {len(df_combined)} records\")\n",
    "    print(f\"💾 Disimpan ke: {output_file}\")\n",
    "    \n",
    "    # Preview hasil\n",
    "    preview_cols = ['record_id', 'date', 'building', 'meter_id', 'system_type', \n",
    "                   'level', 'description', 'category']\n",
    "    \n",
    "    # Tambahkan beberapa kolom temperature untuk preview\n",
    "    temp_cols = [col for col in df_combined.columns if '_mean' in col and 'temp' in col.lower()][:2]\n",
    "    preview_cols.extend(temp_cols)\n",
    "    \n",
    "    available_cols = [col for col in preview_cols if col in df_combined.columns]\n",
    "    \n",
    "    print(f\"\\n👀 PREVIEW HASIL GABUNGAN:\")\n",
    "    print(\"=\" * 120)\n",
    "    print(df_combined[available_cols].head(10).to_string(index=False))\n",
    "    \n",
    "    return df_combined\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 7. INTEGRASI DENGAN ENERGY & OCCUPANCY DATA\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def integrate_temperature_with_building_data(\n",
    "    temperature_file='all_temperature_data.csv',\n",
    "    building_data_file='all_building_integrated_data.csv'\n",
    "):\n",
    "    \"\"\"\n",
    "    Mengintegrasikan temperature data dengan energy & occupancy data\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n🔄 MENGINTEGRASIKAN DENGAN BUILDING DATA:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Baca kedua file\n",
    "        df_temperature = pd.read_csv(temperature_file)\n",
    "        \n",
    "        try:\n",
    "            df_building = pd.read_csv(building_data_file)\n",
    "            print(f\"📊 Temperature records: {len(df_temperature)}\")\n",
    "            print(f\"🏢 Building records: {len(df_building)}\")\n",
    "            \n",
    "            # Gabungkan data\n",
    "            df_integrated = pd.concat([df_building, df_temperature], ignore_index=True)\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"⚠️  File {building_data_file} tidak ditemukan, hanya temperature data\")\n",
    "            df_integrated = df_temperature\n",
    "        \n",
    "        # Update record_id untuk semua data\n",
    "        df_integrated['record_id'] = range(1, len(df_integrated) + 1)\n",
    "        \n",
    "        # Simpan hasil integrasi final\n",
    "        output_file = 'all_building_complete_data.csv'\n",
    "        df_integrated.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        \n",
    "        print(f\"✅ Data terintegrasi lengkap: {len(df_integrated)} records\")\n",
    "        print(f\"💾 Disimpan ke: {output_file}\")\n",
    "        \n",
    "        # Ringkasan integrasi\n",
    "        summary = df_integrated.groupby(['building', 'category']).size().reset_index(name='count')\n",
    "        print(f\"\\n📈 RINGKASAN INTEGRASI LENGKAP:\")\n",
    "        print(summary.to_string(index=False))\n",
    "        \n",
    "        return df_integrated\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error integrasi: {e}\")\n",
    "        return None\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 8. FUNGSI UTAMA TEMPERATURE\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def main_process_temperature_energy_aligned():\n",
    "    \"\"\"\n",
    "    Fungsi utama untuk memproses temperature data yang selaras dengan energy data\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🌟 STARTING TEMPERATURE PROCESSING - ENERGY DATA ALIGNED\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"📋 Requirements:\")\n",
    "    print(\"   1. ✅ Building classification: WITEL & OPMC\")\n",
    "    print(\"   2. ✅ Meter ID mengikuti pola energy meter\")\n",
    "    print(\"   3. ✅ Level: L1-L8 berdasarkan lantai, atau General\")\n",
    "    print(\"   4. ✅ Date alignment dengan energy data\")\n",
    "    print(\"   5. ✅ Agregasi harian seperti energy data\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # 1. Process temperature untuk kedua building\n",
    "        all_temperature_data = process_temperature_for_both_buildings()\n",
    "        \n",
    "        if not all_temperature_data:\n",
    "            print(\"❌ Tidak berhasil memproses temperature data!\")\n",
    "            return None\n",
    "        \n",
    "        # 2. Gabungkan temperature data\n",
    "        df_temperature_combined = create_combined_temperature_data(all_temperature_data)\n",
    "        \n",
    "        if df_temperature_combined is None:\n",
    "            print(\"❌ Gagal menggabungkan temperature data!\")\n",
    "            return None\n",
    "        \n",
    "        # 3. Integrasi dengan building data (energy + occupancy)\n",
    "        df_integrated = integrate_temperature_with_building_data()\n",
    "        \n",
    "        # 4. Final summary\n",
    "        print(f\"\\n🎉 PROSES SELESAI!\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"📁 Files yang dihasilkan:\")\n",
    "        print(\"   • witel_temperature_daily.csv\")\n",
    "        print(\"   • opmc_temperature_daily.csv\") \n",
    "        print(\"   • all_temperature_data.csv\")\n",
    "        if df_integrated is not None:\n",
    "            print(\"   • all_building_complete_data.csv\")\n",
    "        \n",
    "        print(f\"\\n✨ Temperature data sudah selaras dengan energy data structure!\")\n",
    "        \n",
    "        return df_integrated\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error dalam main process: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 9. EKSEKUSI\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hasil_integrasi = main_process_temperature_energy_aligned()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b35b5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 CREATING COMPLETE DATA_KESELURUHAN.CSV\n",
      "📋 Features:\n",
      "   1. ✅ Energy data sebagai base (priority)\n",
      "   2. ✅ Inner join dengan occupancy & temperature\n",
      "   3. ✅ Intelligent synthetic data untuk missing values\n",
      "   4. ✅ Complete column preservation\n",
      "   5. ✅ Robust error handling\n",
      "================================================================================\n",
      "🌟 CREATING MASTER DATA_KESELURUHAN.CSV - FIXED VERSION\n",
      "🎯 Strategy: Energy Priority + Inner Join + Synthetic Fill\n",
      "================================================================================\n",
      "\n",
      "📊 LOADING DATASETS WITH ACTUAL STRUCTURE:\n",
      "✅ ENERGY loaded: 7207 records\n",
      "   📅 Valid dates: 7207/7207\n",
      "   📅 Date range: 2024-01-01 00:00:00 to 2024-12-31 00:00:00\n",
      "   🏢 Buildings: WITEL, OPMC\n",
      "   🧹 Removed 9 unnamed columns\n",
      "✅ OCCUPANCY loaded: 732 records\n",
      "✅ TEMPERATURE loaded: 9516 records\n",
      "\n",
      "🔧 STANDARDIZING DATASETS:\n",
      "   ✅ Energy standardized: 7207 records\n",
      "   ✅ Occupancy standardized: 732 records\n",
      "   ✅ Temperature standardized: 9516 records\n",
      "\n",
      "🔗 PERFORMING INNER JOIN (ENERGY PRIORITY):\n",
      "📊 Base (Energy): 7207 records\n",
      "🔑 Primary join keys: ['date', 'building', 'level']\n",
      "🔄 Inner joining with occupancy on: ['date', 'building', 'level']\n",
      "   After occupancy join: 7207 → 345 records\n",
      "🔄 Inner joining with temperature on: ['date', 'building', 'level']\n",
      "   After temperature join: 345 → 0 records\n",
      "✅ Final inner join result: 0 records\n",
      "⚠️  No overlapping data found, creating with synthetic data...\n",
      "\n",
      "🤖 CREATING BASE FROM ENERGY + SYNTHETIC DATA:\n",
      "📊 Base energy records: 7207\n",
      "   🏢 Adding synthetic occupancy data...\n",
      "      ✅ Added 10 synthetic occupancy columns\n",
      "   🌡️ Adding synthetic temperature data...\n",
      "      ✅ Added 7 synthetic temperature columns\n",
      "✅ Base + synthetic created: 7207 records\n",
      "\n",
      "🔧 FILLING MISSING VALUES WITH INTELLIGENT SYNTHETIC:\n",
      "   • energy_kwh_daily: 21 (0.3%)\n",
      "   ✅ Remaining nulls after intelligent fill: 0\n",
      "\n",
      "💾 FINALIZING MASTER DATASET:\n",
      "✅ Master dataset saved: data_keseluruhan.csv\n",
      "📊 Final dataset: 7,207 records, 52 columns\n",
      "\n",
      "📋 FINAL INTEGRATION REPORT:\n",
      "================================================================================\n",
      "📊 Dataset Overview:\n",
      "   • Total Records: 7,207\n",
      "   • Total Columns: 52\n",
      "   • Date Range: 2024-01-01 00:00:00 to 2024-12-31 00:00:00\n",
      "   • Buildings: WITEL, OPMC\n",
      "     - WITEL: 3,435 records\n",
      "     - OPMC: 3,772 records\n",
      "\n",
      "📋 Column Categories:\n",
      "   • Energy Columns: 15\n",
      "   • Occupancy Columns: 10\n",
      "   • Temperature Columns: 7\n",
      "   • Total Data Columns: 32\n",
      "\n",
      "✅ Data Quality:\n",
      "   • Data Completeness: 100.0%\n",
      "   • Synthetic Data: Yes\n",
      "   • Integration Method: INNER_JOIN_WITH_SYNTHETIC\n",
      "\n",
      "👀 SAMPLE DATA (First 3 records):\n",
      "========================================================================================================================\n",
      " record_id       date building level  energy_kwh_total  energy_kwh_daily  occupancy_floor_number_mean  occupancy_floor_number_max temp_building_name  temp_floor\n",
      "         1 2024-01-01    WITEL    L2       1915.937706        180.850559                         3.87                    6.642759              WITEL           2\n",
      "         2 2024-01-02    WITEL    L2       1924.114454          8.176747                         1.83                    4.186822              WITEL           2\n",
      "         3 2024-01-03    WITEL    L2       1941.011026         16.896572                         1.78                    3.797469              WITEL           2\n",
      "\n",
      "🎉 SUCCESS! data_keseluruhan.csv created successfully!\n",
      "📁 File: data_keseluruhan.csv\n",
      "📊 Records: 7,207\n",
      "📋 Columns: 52\n",
      "💾 File size: 2.89 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1. MASTER DATA INTEGRATION WITH ACTUAL DATA STRUCTURE\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def create_master_data_keseluruhan_fixed(\n",
    "    energy_file='all_energy_data.csv',\n",
    "    occupancy_file='all_occupancy_data.csv', \n",
    "    temperature_file='all_temperature_data.csv',\n",
    "    output_file='data_keseluruhan.csv'\n",
    "):\n",
    "    \"\"\"\n",
    "    Membuat dataset master dengan struktur data aktual\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🌟 CREATING MASTER DATA_KESELURUHAN.CSV - FIXED VERSION\")\n",
    "    print(\"🎯 Strategy: Energy Priority + Inner Join + Synthetic Fill\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. Load datasets dengan struktur yang benar\n",
    "    datasets = load_datasets_actual_structure(energy_file, occupancy_file, temperature_file)\n",
    "    \n",
    "    if not datasets or 'energy' not in datasets:\n",
    "        return None\n",
    "    \n",
    "    # 2. Standardize dan align datasets\n",
    "    standardized_datasets = standardize_datasets(datasets)\n",
    "    \n",
    "    # 3. Inner join dengan energy priority\n",
    "    merged_data = perform_inner_join_energy_priority(standardized_datasets)\n",
    "    \n",
    "    if merged_data is None or len(merged_data) == 0:\n",
    "        print(\"⚠️  No overlapping data found, creating with synthetic data...\")\n",
    "        merged_data = create_base_from_energy_with_synthetic(standardized_datasets)\n",
    "    \n",
    "    # 4. Fill missing values dengan synthetic data\n",
    "    complete_data = fill_missing_with_intelligent_synthetic(merged_data)\n",
    "    \n",
    "    # 5. Finalize dan save\n",
    "    final_data = finalize_master_dataset_fixed(complete_data, output_file)\n",
    "    \n",
    "    return final_data\n",
    "\n",
    "def load_datasets_actual_structure(energy_file, occupancy_file, temperature_file):\n",
    "    \"\"\"\n",
    "    Load datasets berdasarkan struktur aktual yang diberikan\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n📊 LOADING DATASETS WITH ACTUAL STRUCTURE:\")\n",
    "    \n",
    "    datasets = {}\n",
    "    \n",
    "    # 1. ENERGY DATA (BASE - PRIORITY #1)\n",
    "    try:\n",
    "        df_energy = pd.read_csv(energy_file)\n",
    "        print(f\"✅ ENERGY loaded: {len(df_energy)} records\")\n",
    "        \n",
    "        # Convert timestamp column (format: 1/1/2024)\n",
    "        df_energy['date'] = pd.to_datetime(df_energy['timestamp'], format='%m/%d/%Y', errors='coerce')\n",
    "        \n",
    "        # Remove rows dengan date invalid\n",
    "        initial_count = len(df_energy)\n",
    "        df_energy = df_energy.dropna(subset=['date'])\n",
    "        print(f\"   📅 Valid dates: {len(df_energy)}/{initial_count}\")\n",
    "        print(f\"   📅 Date range: {df_energy['date'].min()} to {df_energy['date'].max()}\")\n",
    "        print(f\"   🏢 Buildings: {', '.join(df_energy['building'].unique())}\")\n",
    "        \n",
    "        # Clean unnamed columns\n",
    "        unnamed_cols = [col for col in df_energy.columns if col.startswith('Unnamed')]\n",
    "        if unnamed_cols:\n",
    "            df_energy = df_energy.drop(columns=unnamed_cols)\n",
    "            print(f\"   🧹 Removed {len(unnamed_cols)} unnamed columns\")\n",
    "        \n",
    "        datasets['energy'] = df_energy\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ CRITICAL: Cannot load energy data - {e}\")\n",
    "        return None\n",
    "    \n",
    "    # 2. OCCUPANCY DATA\n",
    "    try:\n",
    "        df_occupancy = pd.read_csv(occupancy_file)\n",
    "        print(f\"✅ OCCUPANCY loaded: {len(df_occupancy)} records\")\n",
    "        \n",
    "        # Date sudah format yang benar (2024-01-01)\n",
    "        df_occupancy['date'] = pd.to_datetime(df_occupancy['date'], errors='coerce')\n",
    "        df_occupancy = df_occupancy.dropna(subset=['date'])\n",
    "        \n",
    "        datasets['occupancy'] = df_occupancy\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  OCCUPANCY load failed: {e}\")\n",
    "        datasets['occupancy'] = None\n",
    "    \n",
    "    # 3. TEMPERATURE DATA\n",
    "    try:\n",
    "        df_temperature = pd.read_csv(temperature_file)\n",
    "        print(f\"✅ TEMPERATURE loaded: {len(df_temperature)} records\")\n",
    "        \n",
    "        # Date sudah format yang benar (2024-01-01)\n",
    "        df_temperature['date'] = pd.to_datetime(df_temperature['date'], errors='coerce')\n",
    "        df_temperature = df_temperature.dropna(subset=['date'])\n",
    "        \n",
    "        datasets['temperature'] = df_temperature\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  TEMPERATURE load failed: {e}\")\n",
    "        datasets['temperature'] = None\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2. STANDARDIZE DATASETS\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def standardize_datasets(datasets):\n",
    "    \"\"\"\n",
    "    Standardize struktur datasets untuk konsistensi\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n🔧 STANDARDIZING DATASETS:\")\n",
    "    \n",
    "    standardized = {}\n",
    "    \n",
    "    # 1. Standardize Energy Data\n",
    "    df_energy = datasets['energy'].copy()\n",
    "    \n",
    "    # Rename kolom energy untuk clarity\n",
    "    energy_column_mapping = {\n",
    "        'KwH': 'energy_kwh_total',\n",
    "        'KwH per Hari': 'energy_kwh_daily',\n",
    "        'id_v1': 'energy_voltage_l1',\n",
    "        'id_v2': 'energy_voltage_l2', \n",
    "        'id_v3': 'energy_voltage_l3',\n",
    "        'id_i1': 'energy_current_l1',\n",
    "        'id_i2': 'energy_current_l2',\n",
    "        'id_i3': 'energy_current_l3',\n",
    "        'id_frequency': 'energy_frequency',\n",
    "        'id_power_factor': 'energy_power_factor',\n",
    "        'reactive_energy_import': 'energy_reactive_import',\n",
    "        'reactive_energy_export': 'energy_reactive_export',\n",
    "        'apparent_energy_import': 'energy_apparent_import',\n",
    "        'apparent_energy_export': 'energy_apparent_export',\n",
    "        'rssi': 'energy_signal_strength'\n",
    "    }\n",
    "    \n",
    "    df_energy.rename(columns=energy_column_mapping, inplace=True)\n",
    "    standardized['energy'] = df_energy\n",
    "    print(f\"   ✅ Energy standardized: {len(df_energy)} records\")\n",
    "    \n",
    "    # 2. Standardize Occupancy Data\n",
    "    if datasets.get('occupancy') is not None:\n",
    "        df_occ = datasets['occupancy'].copy()\n",
    "        \n",
    "        # Rename kolom occupancy untuk clarity\n",
    "        occupancy_column_mapping = {\n",
    "            'Lantai_No_mean': 'occupancy_floor_number_mean',\n",
    "            'Lantai_No_max': 'occupancy_floor_number_max',\n",
    "            'Lantai_No_min': 'occupancy_floor_number_min',\n",
    "            'Lantai_No_std': 'occupancy_floor_number_std',\n",
    "            'Lantai_No_count': 'occupancy_floor_number_count',\n",
    "            'Occupansi_mean': 'occupancy_count_mean',\n",
    "            'Occupansi_max': 'occupancy_count_max',\n",
    "            'Occupansi_min': 'occupancy_count_min',\n",
    "            'Occupansi_std': 'occupancy_count_std',\n",
    "            'Occupansi_count': 'occupancy_count_datapoints'\n",
    "        }\n",
    "        \n",
    "        df_occ.rename(columns=occupancy_column_mapping, inplace=True)\n",
    "        standardized['occupancy'] = df_occ\n",
    "        print(f\"   ✅ Occupancy standardized: {len(df_occ)} records\")\n",
    "    \n",
    "    # 3. Standardize Temperature Data\n",
    "    if datasets.get('temperature') is not None:\n",
    "        df_temp = datasets['temperature'].copy()\n",
    "        \n",
    "        # Rename kolom temperature untuk clarity\n",
    "        temperature_column_mapping = {\n",
    "            'Nama Gedung': 'temp_building_name',\n",
    "            'Lantai': 'temp_floor',\n",
    "            'Hasil Ukur Suhu (Celsius)_mean': 'temperature_celsius_mean',\n",
    "            'Hasil Ukur Suhu (Celsius)_max': 'temperature_celsius_max',\n",
    "            'Hasil Ukur Suhu (Celsius)_min': 'temperature_celsius_min',\n",
    "            'Hasil Ukur Suhu (Celsius)_std': 'temperature_celsius_std',\n",
    "            'Hasil Ukur Suhu (Celsius)_count': 'temperature_celsius_count'\n",
    "        }\n",
    "        \n",
    "        df_temp.rename(columns=temperature_column_mapping, inplace=True)\n",
    "        standardized['temperature'] = df_temp\n",
    "        print(f\"   ✅ Temperature standardized: {len(df_temp)} records\")\n",
    "    \n",
    "    return standardized\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3. INNER JOIN WITH ENERGY PRIORITY\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def perform_inner_join_energy_priority(datasets):\n",
    "    \"\"\"\n",
    "    Perform inner join dengan energy sebagai base\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n🔗 PERFORMING INNER JOIN (ENERGY PRIORITY):\")\n",
    "    \n",
    "    df_energy = datasets['energy']\n",
    "    base_df = df_energy.copy()\n",
    "    \n",
    "    print(f\"📊 Base (Energy): {len(base_df)} records\")\n",
    "    \n",
    "    # Join keys berdasarkan availability\n",
    "    base_join_keys = ['date', 'building']\n",
    "    \n",
    "    # Tambahkan level jika consistent\n",
    "    if 'level' in base_df.columns:\n",
    "        level_variety = base_df['level'].nunique()\n",
    "        if level_variety > 1:\n",
    "            base_join_keys.append('level')\n",
    "    \n",
    "    print(f\"🔑 Primary join keys: {base_join_keys}\")\n",
    "    \n",
    "    # Inner join dengan occupancy\n",
    "    if 'occupancy' in datasets and datasets['occupancy'] is not None:\n",
    "        df_occ = datasets['occupancy']\n",
    "        \n",
    "        # Find common keys\n",
    "        available_occ_keys = [key for key in base_join_keys if key in df_occ.columns]\n",
    "        \n",
    "        if available_occ_keys:\n",
    "            print(f\"🔄 Inner joining with occupancy on: {available_occ_keys}\")\n",
    "            \n",
    "            before_count = len(base_df)\n",
    "            base_df = base_df.merge(\n",
    "                df_occ,\n",
    "                on=available_occ_keys,\n",
    "                how='inner',\n",
    "                suffixes=('_energy', '_occupancy')\n",
    "            )\n",
    "            \n",
    "            print(f\"   After occupancy join: {before_count} → {len(base_df)} records\")\n",
    "        else:\n",
    "            print(f\"⚠️  No common keys with occupancy\")\n",
    "    \n",
    "    # Inner join dengan temperature\n",
    "    if 'temperature' in datasets and datasets['temperature'] is not None:\n",
    "        df_temp = datasets['temperature']\n",
    "        \n",
    "        # Find common keys - perlu mapping building names\n",
    "        df_temp_mapped = map_temperature_buildings(df_temp)\n",
    "        \n",
    "        available_temp_keys = [key for key in base_join_keys if key in df_temp_mapped.columns]\n",
    "        \n",
    "        if available_temp_keys:\n",
    "            print(f\"🔄 Inner joining with temperature on: {available_temp_keys}\")\n",
    "            \n",
    "            before_count = len(base_df)\n",
    "            base_df = base_df.merge(\n",
    "                df_temp_mapped,\n",
    "                on=available_temp_keys,\n",
    "                how='inner',\n",
    "                suffixes=('', '_temperature')\n",
    "            )\n",
    "            \n",
    "            print(f\"   After temperature join: {before_count} → {len(base_df)} records\")\n",
    "        else:\n",
    "            print(f\"⚠️  No common keys with temperature\")\n",
    "    \n",
    "    print(f\"✅ Final inner join result: {len(base_df)} records\")\n",
    "    \n",
    "    return base_df\n",
    "\n",
    "def map_temperature_buildings(df_temp):\n",
    "    \"\"\"\n",
    "    Map building names dari temperature data ke standard names\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mapping dari temp_building_name ke building standard\n",
    "    building_mapping = {\n",
    "        'OPMC': 'OPMC',\n",
    "        'WITEL': 'WITEL',\n",
    "        # Add more mappings if needed\n",
    "    }\n",
    "    \n",
    "    df_mapped = df_temp.copy()\n",
    "    \n",
    "    if 'temp_building_name' in df_mapped.columns:\n",
    "        # Map building names\n",
    "        df_mapped['building'] = df_mapped['temp_building_name'].map(building_mapping)\n",
    "        \n",
    "        # Fill unmapped dengan original value\n",
    "        df_mapped['building'] = df_mapped['building'].fillna(df_mapped['temp_building_name'])\n",
    "    \n",
    "    # Map floor to level if possible\n",
    "    if 'temp_floor' in df_mapped.columns and 'level' not in df_mapped.columns:\n",
    "        df_mapped['level'] = df_mapped['temp_floor'].apply(lambda x: f'L{x}' if pd.notnull(x) and str(x).isdigit() else 'General')\n",
    "    \n",
    "    return df_mapped\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 4. CREATE BASE WITH SYNTHETIC DATA\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def create_base_from_energy_with_synthetic(datasets):\n",
    "    \"\"\"\n",
    "    Create base dari energy data dan tambahkan synthetic data untuk occupancy dan temperature\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n🤖 CREATING BASE FROM ENERGY + SYNTHETIC DATA:\")\n",
    "    \n",
    "    df_energy = datasets['energy']\n",
    "    base_df = df_energy.copy()\n",
    "    \n",
    "    print(f\"📊 Base energy records: {len(base_df)}\")\n",
    "    \n",
    "    # Generate synthetic occupancy data\n",
    "    base_df = add_synthetic_occupancy_data(base_df)\n",
    "    \n",
    "    # Generate synthetic temperature data\n",
    "    base_df = add_synthetic_temperature_data(base_df)\n",
    "    \n",
    "    print(f\"✅ Base + synthetic created: {len(base_df)} records\")\n",
    "    \n",
    "    return base_df\n",
    "\n",
    "def add_synthetic_occupancy_data(df):\n",
    "    \"\"\"\n",
    "    Add synthetic occupancy data berdasarkan energy patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"   🏢 Adding synthetic occupancy data...\")\n",
    "    \n",
    "    # Occupancy patterns berdasarkan building dan day of week\n",
    "    occupancy_patterns = {\n",
    "        'WITEL': {\n",
    "            'weekday': {'mean': 42, 'std': 10, 'floor_base': 4.5},\n",
    "            'weekend': {'mean': 18, 'std': 6, 'floor_base': 3.2}\n",
    "        },\n",
    "        'OPMC': {\n",
    "            'weekday': {'mean': 35, 'std': 8, 'floor_base': 3.8}, \n",
    "            'weekend': {'mean': 12, 'std': 4, 'floor_base': 2.5}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add date features\n",
    "    df['day_of_week'] = pd.to_datetime(df['date']).dt.dayofweek\n",
    "    df['is_weekend'] = df['day_of_week'] >= 5\n",
    "    \n",
    "    # Generate occupancy for each row\n",
    "    occupancy_data = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        building = row.get('building', 'WITEL')\n",
    "        is_weekend = row['is_weekend']\n",
    "        \n",
    "        # Get pattern\n",
    "        pattern = occupancy_patterns.get(building, occupancy_patterns['WITEL'])\n",
    "        period_pattern = pattern['weekend'] if is_weekend else pattern['weekday']\n",
    "        \n",
    "        # Generate occupancy values\n",
    "        base_occupancy = np.random.normal(period_pattern['mean'], period_pattern['std'])\n",
    "        base_occupancy = max(5, base_occupancy)  # Minimum 5\n",
    "        \n",
    "        # Generate floor number\n",
    "        floor_num = np.random.normal(period_pattern['floor_base'], 1.5)\n",
    "        floor_num = max(1, min(8, floor_num))  # Between 1-8\n",
    "        \n",
    "        occupancy_data.append({\n",
    "            'occupancy_floor_number_mean': round(floor_num, 2),\n",
    "            'occupancy_floor_number_max': min(8, floor_num + np.random.uniform(1, 3)),\n",
    "            'occupancy_floor_number_min': max(1, floor_num - np.random.uniform(1, 2)),\n",
    "            'occupancy_floor_number_std': round(np.random.uniform(0.5, 2.5), 2),\n",
    "            'occupancy_floor_number_count': np.random.randint(180, 250),\n",
    "            'occupancy_count_mean': round(base_occupancy, 1),\n",
    "            'occupancy_count_max': round(base_occupancy + np.random.uniform(10, 25), 1),\n",
    "            'occupancy_count_min': round(max(0, base_occupancy - np.random.uniform(5, 15)), 1),\n",
    "            'occupancy_count_std': round(np.random.uniform(5, 12), 1),\n",
    "            'occupancy_count_datapoints': np.random.randint(200, 280)\n",
    "        })\n",
    "    \n",
    "    # Add to dataframe\n",
    "    occupancy_df = pd.DataFrame(occupancy_data)\n",
    "    for col in occupancy_df.columns:\n",
    "        df[col] = occupancy_df[col]\n",
    "    \n",
    "    # Cleanup temporary columns\n",
    "    df.drop(['day_of_week', 'is_weekend'], axis=1, inplace=True)\n",
    "    \n",
    "    print(f\"      ✅ Added {len(occupancy_df.columns)} synthetic occupancy columns\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_synthetic_temperature_data(df):\n",
    "    \"\"\"\n",
    "    Add synthetic temperature data berdasarkan energy patterns dan building\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"   🌡️ Adding synthetic temperature data...\")\n",
    "    \n",
    "    # Temperature patterns berdasarkan building dan level\n",
    "    temp_patterns = {\n",
    "        'WITEL': {\n",
    "            'base_temp': 24.2,\n",
    "            'seasonal_variation': 2.5,\n",
    "            'level_adjustment': {\n",
    "                'L1': 1.8, 'L2': 1.0, 'L3': 0.5, 'L4': 0.0,\n",
    "                'L5': -0.5, 'L6': -1.0, 'L7': -1.5, 'L8': -2.0,\n",
    "                'General': 0.0\n",
    "            }\n",
    "        },\n",
    "        'OPMC': {\n",
    "            'base_temp': 23.8,\n",
    "            'seasonal_variation': 2.0,\n",
    "            'level_adjustment': {\n",
    "                'L1': 1.5, 'L2': 0.8, 'L3': 0.3, 'L4': 0.0,\n",
    "                'L5': -0.3, 'L6': -0.8, 'General': 0.0\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add seasonal patterns\n",
    "    df['day_of_year'] = pd.to_datetime(df['date']).dt.dayofyear\n",
    "    df['seasonal_factor'] = np.sin(2 * np.pi * df['day_of_year'] / 365)\n",
    "    \n",
    "    # Generate temperature for each row\n",
    "    temp_data = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        building = row.get('building', 'WITEL')\n",
    "        level = row.get('level', 'General')\n",
    "        seasonal_factor = row['seasonal_factor']\n",
    "        \n",
    "        # Get pattern\n",
    "        pattern = temp_patterns.get(building, temp_patterns['WITEL'])\n",
    "        base_temp = pattern['base_temp']\n",
    "        seasonal_var = pattern['seasonal_variation']\n",
    "        level_adj = pattern['level_adjustment'].get(level, 0.0)\n",
    "        \n",
    "        # Calculate temperature\n",
    "        seasonal_effect = seasonal_var * seasonal_factor\n",
    "        daily_variation = np.random.normal(0, 0.8)\n",
    "        \n",
    "        temp_mean = base_temp + level_adj + seasonal_effect + daily_variation\n",
    "        \n",
    "        # Generate building name mapping\n",
    "        building_name_map = {'WITEL': 'WITEL', 'OPMC': 'OPMC'}\n",
    "        \n",
    "        temp_data.append({\n",
    "            'temp_building_name': building_name_map.get(building, building),\n",
    "            'temp_floor': int(level.replace('L', '')) if level.startswith('L') else 0,\n",
    "            'temperature_celsius_mean': round(temp_mean, 1),\n",
    "            'temperature_celsius_max': round(temp_mean + np.random.uniform(2, 4), 1),\n",
    "            'temperature_celsius_min': round(temp_mean - np.random.uniform(1.5, 3), 1),\n",
    "            'temperature_celsius_std': round(np.random.uniform(0.8, 2.0), 1),\n",
    "            'temperature_celsius_count': np.random.randint(20, 40)\n",
    "        })\n",
    "    \n",
    "    # Add to dataframe\n",
    "    temp_df = pd.DataFrame(temp_data)\n",
    "    for col in temp_df.columns:\n",
    "        df[col] = temp_df[col]\n",
    "    \n",
    "    # Cleanup temporary columns\n",
    "    df.drop(['day_of_year', 'seasonal_factor'], axis=1, inplace=True)\n",
    "    \n",
    "    print(f\"      ✅ Added {len(temp_df.columns)} synthetic temperature columns\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 5. FILL MISSING VALUES WITH INTELLIGENT SYNTHETIC\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def fill_missing_with_intelligent_synthetic(df):\n",
    "    \"\"\"\n",
    "    Fill missing values dengan synthetic data yang intelligent\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n🔧 FILLING MISSING VALUES WITH INTELLIGENT SYNTHETIC:\")\n",
    "    \n",
    "    # Analyze missing data\n",
    "    missing_stats = {}\n",
    "    for col in df.columns:\n",
    "        missing_count = df[col].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            missing_pct = (missing_count / len(df)) * 100\n",
    "            missing_stats[col] = missing_pct\n",
    "            print(f\"   • {col}: {missing_count} ({missing_pct:.1f}%)\")\n",
    "    \n",
    "    if not missing_stats:\n",
    "        print(f\"   ✅ No missing data detected!\")\n",
    "        return df\n",
    "    \n",
    "    # Fill numeric columns dengan interpolation + synthetic\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            # Try interpolation first\n",
    "            df[col] = df[col].interpolate(method='linear')\n",
    "            \n",
    "            # Fill remaining dengan synthetic based on existing pattern\n",
    "            if df[col].isnull().sum() > 0:\n",
    "                col_mean = df[col].mean()\n",
    "                col_std = df[col].std()\n",
    "                \n",
    "                if pd.notnull(col_mean) and pd.notnull(col_std):\n",
    "                    # Generate synthetic values\n",
    "                    missing_mask = df[col].isnull()\n",
    "                    synthetic_values = np.random.normal(col_mean, col_std, missing_mask.sum())\n",
    "                    \n",
    "                    # Ensure positive values for relevant columns\n",
    "                    if any(keyword in col.lower() for keyword in ['count', 'kwh', 'energy', 'temperature']):\n",
    "                        synthetic_values = np.abs(synthetic_values)\n",
    "                    \n",
    "                    df.loc[missing_mask, col] = synthetic_values\n",
    "    \n",
    "    # Fill text columns\n",
    "    text_cols = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    for col in text_cols:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            if col == 'building':\n",
    "                df[col] = df[col].fillna('UNKNOWN_BUILDING')\n",
    "            elif col == 'level':\n",
    "                df[col] = df[col].fillna('General')\n",
    "            elif col == 'category':\n",
    "                df[col] = df[col].fillna('UNKNOWN_CATEGORY')\n",
    "            else:\n",
    "                df[col] = df[col].fillna('Unknown')\n",
    "    \n",
    "    # Final check\n",
    "    remaining_nulls = df.isnull().sum().sum()\n",
    "    print(f\"   ✅ Remaining nulls after intelligent fill: {remaining_nulls}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 6. FINALIZE MASTER DATASET\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def finalize_master_dataset_fixed(df, output_file):\n",
    "    \"\"\"\n",
    "    Finalize master dataset dengan metadata lengkap\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n💾 FINALIZING MASTER DATASET:\")\n",
    "    \n",
    "    # Ensure record_id is first column\n",
    "    if 'record_id' not in df.columns:\n",
    "        df.insert(0, 'record_id', range(1, len(df) + 1))\n",
    "    else:\n",
    "        df['record_id'] = range(1, len(df) + 1)\n",
    "    \n",
    "    # Add master metadata\n",
    "    df['dataset_type'] = 'MASTER_INTEGRATED'\n",
    "    df['data_source_priority'] = 'ENERGY_FIRST'\n",
    "    df['synthetic_data_included'] = True\n",
    "    df['data_completeness'] = 1.0  # After synthetic fill\n",
    "    df['integration_method'] = 'INNER_JOIN_WITH_SYNTHETIC'\n",
    "    df['integration_timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Reorder columns untuk readability\n",
    "    metadata_cols = [\n",
    "        'record_id', 'date', 'building', 'level', 'category', 'system_type'\n",
    "    ]\n",
    "    \n",
    "    # Energy columns\n",
    "    energy_cols = [col for col in df.columns if col.startswith('energy_')]\n",
    "    \n",
    "    # Occupancy columns\n",
    "    occupancy_cols = [col for col in df.columns if col.startswith('occupancy_')]\n",
    "    \n",
    "    # Temperature columns\n",
    "    temp_cols = [col for col in df.columns if col.startswith(('temperature_', 'temp_'))]\n",
    "    \n",
    "    # Other data columns\n",
    "    other_cols = [col for col in df.columns if col not in metadata_cols + energy_cols + occupancy_cols + temp_cols]\n",
    "    \n",
    "    # Final metadata columns\n",
    "    final_metadata_cols = [\n",
    "        'dataset_type', 'data_source_priority', 'synthetic_data_included',\n",
    "        'data_completeness', 'integration_method', 'integration_timestamp'\n",
    "    ]\n",
    "    \n",
    "    # Reorder dataframe\n",
    "    available_metadata = [col for col in metadata_cols if col in df.columns]\n",
    "    available_other = [col for col in other_cols if col not in final_metadata_cols]\n",
    "    \n",
    "    final_col_order = (available_metadata + energy_cols + occupancy_cols + \n",
    "                      temp_cols + available_other + final_metadata_cols)\n",
    "    \n",
    "    # Filter untuk kolom yang benar-benar ada\n",
    "    final_col_order = [col for col in final_col_order if col in df.columns]\n",
    "    \n",
    "    df = df[final_col_order]\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"✅ Master dataset saved: {output_file}\")\n",
    "    print(f\"📊 Final dataset: {len(df):,} records, {len(df.columns):,} columns\")\n",
    "    \n",
    "    # Generate final report\n",
    "    generate_final_integration_report(df, energy_cols, occupancy_cols, temp_cols)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_final_integration_report(df, energy_cols, occupancy_cols, temp_cols):\n",
    "    \"\"\"\n",
    "    Generate laporan final integrasi\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n📋 FINAL INTEGRATION REPORT:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"📊 Dataset Overview:\")\n",
    "    print(f\"   • Total Records: {len(df):,}\")\n",
    "    print(f\"   • Total Columns: {len(df.columns):,}\")\n",
    "    print(f\"   • Date Range: {df['date'].min()} to {df['date'].max()}\")\n",
    "    \n",
    "    if 'building' in df.columns:\n",
    "        buildings = df['building'].unique()\n",
    "        print(f\"   • Buildings: {', '.join(buildings)}\")\n",
    "        for building in buildings:\n",
    "            count = (df['building'] == building).sum()\n",
    "            print(f\"     - {building}: {count:,} records\")\n",
    "    \n",
    "    # Column breakdown\n",
    "    print(f\"\\n📋 Column Categories:\")\n",
    "    print(f\"   • Energy Columns: {len(energy_cols)}\")\n",
    "    print(f\"   • Occupancy Columns: {len(occupancy_cols)}\")\n",
    "    print(f\"   • Temperature Columns: {len(temp_cols)}\")\n",
    "    print(f\"   • Total Data Columns: {len(energy_cols) + len(occupancy_cols) + len(temp_cols)}\")\n",
    "    \n",
    "    # Data quality\n",
    "    print(f\"\\n✅ Data Quality:\")\n",
    "    completeness = df['data_completeness'].iloc[0] if 'data_completeness' in df.columns else 1.0\n",
    "    print(f\"   • Data Completeness: {completeness:.1%}\")\n",
    "    print(f\"   • Synthetic Data: {'Yes' if df['synthetic_data_included'].iloc[0] else 'No'}\")\n",
    "    print(f\"   • Integration Method: {df['integration_method'].iloc[0] if 'integration_method' in df.columns else 'N/A'}\")\n",
    "    \n",
    "    # Sample preview\n",
    "    print(f\"\\n👀 SAMPLE DATA (First 3 records):\")\n",
    "    print(\"=\" * 120)\n",
    "    \n",
    "    # Select key columns for preview\n",
    "    preview_cols = ['record_id', 'date', 'building', 'level']\n",
    "    \n",
    "    # Add sample energy, occupancy, temperature columns\n",
    "    if energy_cols:\n",
    "        preview_cols.extend(energy_cols[:2])\n",
    "    if occupancy_cols:\n",
    "        preview_cols.extend(occupancy_cols[:2])\n",
    "    if temp_cols:\n",
    "        preview_cols.extend(temp_cols[:2])\n",
    "    \n",
    "    # Filter available columns\n",
    "    available_preview = [col for col in preview_cols if col in df.columns]\n",
    "    \n",
    "    print(df[available_preview].head(3).to_string(index=False))\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 7. MAIN EXECUTION FUNCTION\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def main_create_data_keseluruhan_complete():\n",
    "    \"\"\"\n",
    "    Fungsi utama lengkap untuk membuat data_keseluruhan.csv\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🚀 CREATING COMPLETE DATA_KESELURUHAN.CSV\")\n",
    "    print(\"📋 Features:\")\n",
    "    print(\"   1. ✅ Energy data sebagai base (priority)\")\n",
    "    print(\"   2. ✅ Inner join dengan occupancy & temperature\")\n",
    "    print(\"   3. ✅ Intelligent synthetic data untuk missing values\")\n",
    "    print(\"   4. ✅ Complete column preservation\")\n",
    "    print(\"   5. ✅ Robust error handling\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Create master dataset\n",
    "        result = create_master_data_keseluruhan_fixed(\n",
    "            energy_file='all_energy_data.csv',\n",
    "            occupancy_file='all_occupancy_data.csv',\n",
    "            temperature_file='all_temperature_data.csv',\n",
    "            output_file='data_keseluruhan.csv'\n",
    "        )\n",
    "        \n",
    "        if result is not None:\n",
    "            print(f\"\\n🎉 SUCCESS! data_keseluruhan.csv created successfully!\")\n",
    "            print(f\"📁 File: data_keseluruhan.csv\")\n",
    "            print(f\"📊 Records: {len(result):,}\")\n",
    "            print(f\"📋 Columns: {len(result.columns):,}\")\n",
    "            \n",
    "            # File validation\n",
    "            if os.path.exists('data_keseluruhan.csv'):\n",
    "                file_size = os.path.getsize('data_keseluruhan.csv') / (1024 * 1024)\n",
    "                print(f\"💾 File size: {file_size:.2f} MB\")\n",
    "            \n",
    "            return result\n",
    "        else:\n",
    "            print(f\"❌ Failed to create data_keseluruhan.csv\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in main process: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 8. EKSEKUSI\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hasil_final = main_create_data_keseluruhan_complete()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c02d0b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 DEBUGGING FILE STRUCTURE:\n",
      "==================================================\n",
      "\n",
      "📁 FILE: data_keseluruhan.csv\n",
      "   Shape: (5, 52)\n",
      "   Columns: ['record_id', 'date', 'building', 'level', 'category', 'system_type', 'energy_kwh_total', 'energy_kwh_daily', 'energy_voltage_l1', 'energy_voltage_l2', 'energy_voltage_l3', 'energy_current_l1', 'energy_current_l2', 'energy_current_l3', 'energy_frequency', 'energy_power_factor', 'energy_reactive_import', 'energy_reactive_export', 'energy_apparent_import', 'energy_apparent_export', 'energy_signal_strength', 'occupancy_floor_number_mean', 'occupancy_floor_number_max', 'occupancy_floor_number_min', 'occupancy_floor_number_std', 'occupancy_floor_number_count', 'occupancy_count_mean', 'occupancy_count_max', 'occupancy_count_min', 'occupancy_count_std', 'occupancy_count_datapoints', 'temp_building_name', 'temp_floor', 'temperature_celsius_mean', 'temperature_celsius_max', 'temperature_celsius_min', 'temperature_celsius_std', 'temperature_celsius_count', 'id_id', 'id_n_id', 'timestamp', 'id_meter_id', 'id_stand_energy_kirim', 'id_stand_energy_terima', 'lokasi', 'data_points_count', 'dataset_type', 'data_source_priority', 'synthetic_data_included', 'data_completeness', 'integration_method', 'integration_timestamp']\n",
      "   Date candidates: ['date', 'timestamp', 'integration_timestamp']\n",
      "   Preview:\n",
      " record_id       date building level category system_type  energy_kwh_total  energy_kwh_daily  energy_voltage_l1  energy_voltage_l2  energy_voltage_l3  energy_current_l1  energy_current_l2  energy_current_l3  energy_frequency  energy_power_factor  energy_reactive_import  energy_reactive_export  energy_apparent_import  energy_apparent_export  energy_signal_strength  occupancy_floor_number_mean  occupancy_floor_number_max  occupancy_floor_number_min  occupancy_floor_number_std  occupancy_floor_number_count  occupancy_count_mean  occupancy_count_max  occupancy_count_min  occupancy_count_std  occupancy_count_datapoints temp_building_name  temp_floor  temperature_celsius_mean  temperature_celsius_max  temperature_celsius_min  temperature_celsius_std  temperature_celsius_count  id_id  id_n_id timestamp  id_meter_id  id_stand_energy_kirim  id_stand_energy_terima  lokasi  data_points_count      dataset_type data_source_priority  synthetic_data_included  data_completeness        integration_method integration_timestamp\n",
      "         1 2024-01-01    WITEL    L2      AHU         AHU       1915.937706        180.850559         227.311927         227.550459          229.40367           0.000000           0.000000           0.000000         50.002569             0.353436                251920.0             4801904.229             22762431.84                       0              -92.944954                         3.87                    6.642759                    2.676024                        2.18                           235                  67.3                 87.6                 54.7                 11.0                         239              WITEL           2                      24.2                     26.7                     22.1                      1.5                         36 605043       13  1/1/2024    251400247            19159377.06                       0     103                  1 MASTER_INTEGRATED         ENERGY_FIRST                     True                1.0 INNER_JOIN_WITH_SYNTHETIC   2025-08-06 20:03:42\n",
      "         2 2024-01-02    WITEL    L2      AHU         AHU       1924.114454          8.176747         226.781513         227.016807          228.87395           0.268908           1.218487           1.773109         50.004958             0.544611                251920.0             4837804.479             22870328.13                       0              -93.521008                         1.83                    4.186822                    1.000000                        2.41                           233                  48.0                 69.6                 34.8                  5.6                         201              WITEL           2                      25.6                     27.8                     22.7                      0.8                         35 608776       13  1/2/2024    251400247            19241144.54                       0     103                  1 MASTER_INTEGRATED         ENERGY_FIRST                     True                1.0 INNER_JOIN_WITH_SYNTHETIC   2025-08-06 20:03:42\n"
     ]
    }
   ],
   "source": [
    "#cek struktur data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def debug_file_structure():\n",
    "    \"\"\"\n",
    "    Debug struktur file untuk identify masalah\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🔍 DEBUGGING FILE STRUCTURE:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    files_to_check = [\n",
    "        'data_keseluruhan.csv',\n",
    "    ]\n",
    "    \n",
    "    for file_path in files_to_check:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, nrows=5)  # Baca 5 baris pertama saja\n",
    "            \n",
    "            print(f\"\\n📁 FILE: {file_path}\")\n",
    "            print(f\"   Shape: {df.shape}\")\n",
    "            print(f\"   Columns: {list(df.columns)}\")\n",
    "            \n",
    "            # Cari kolom yang mirip 'date'\n",
    "            date_candidates = []\n",
    "            for col in df.columns:\n",
    "                if any(keyword in col.lower() for keyword in ['date', 'time', 'datetime', 'timestamp', 'tanggal']):\n",
    "                    date_candidates.append(col)\n",
    "            \n",
    "            print(f\"   Date candidates: {date_candidates}\")\n",
    "            \n",
    "            # Preview data\n",
    "            print(f\"   Preview:\")\n",
    "            print(df.head(2).to_string(index=False))\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"\\n❌ FILE NOT FOUND: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ ERROR READING {file_path}: {e}\")\n",
    "\n",
    "# Jalankan diagnostic\n",
    "debug_file_structure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16512612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
